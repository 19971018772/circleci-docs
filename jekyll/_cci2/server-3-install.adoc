---
version:
- Server v3.x
- Server Admin
---
= CircleCI Server v3.x Installation Phase 2: Core Services Installation
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

Before you begin with the CircleCI server v3.x installation process, ensure all xref:server-3-install-prerequisites.adoc[prerequisites] are met.

toc::[]

== Install CircleCI server with KOTS
CircleCI server v3.x uses https://kots.io[KOTS] from https://www.replicated.com/[Replicated] to manage and
distribute server v3.x. KOTS is a `kubectl` https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/[plugin].
To install the latest version, you can run `curl https://kots.io/install | bash`.

Ensure you are running the minimum KOTS (KOTS {kotsversion}) by running: 

```
kubectl kots version
```

NOTE: The KOTS command will open up a tunnel to the admin console. If running on Windows inside WSL2, the port is not
available on the host machine. Turning WSL off and back on should resolve the issue. For more information, please see
https://github.com/microsoft/WSL/issues/4199.

From the terminal run (if you are installing behind a proxy see <<Install behind HTTP proxy>> first):

```sh
kubectl kots install circleci-server
```

You will be prompted for a:

* namespace for the deployment
* password for the KOTS admin console

When complete, you should be provided with a URL to access the KOTS admin console, usually: http://localhost:8800.

NOTE: If you need to get back to the management console at a later date you can run: `kubectl kots admin-console -n <namespace kots was installed in>`

NOTE: Once you have created your namespace we recommend setting your `kubectl` context too with the following command: `kubectl config set-context --current --namespace <namespace>`

===== Installing behind HTTP Proxy

If you wish to install CircleCI server behind a proxy, the following command structure should be used (for more information see the KOTS docs https://kots.io/kotsadm/installing/online-install/#proxies[here]): `kubectl kots install circleci-server --http-proxy <<http-proxy-uri>> --https-proxy <<https-proxy> --no-proxy <<no proxy list>>`

The load balancer endpoints must be added to the no-proxy list for the following services: `output processor` and `vm-service`. This is because the no-proxy list is shared between the application and build-agent. The application and build-agent are assumed to be behind the same firewall and therefore cannot have a proxy between them. 

For further information see the <<server-3-operator-proxy#,Configuring a Proxy>> guide.

#INSERT SCREENSHOT#

=== Frontend Settings 
Frontend settings control the web application specific aspects of the CircleCI system. 

Complete the following fields: 

*Domain Name (required)* - Enter the domain name you specified when creating your Frontend TLS key and certificate. 

*Frontend TLS Private Key (recommended)* - You created this during your pre-requisite steps. You can retrieve this value with the following command: `$ cat /etc/letsencrypt/live/<<CIRCLECI_SERVER_DOMAIN>>/privkey.pem`.

*Frontend TLS Certificate (recommended)* - You created this during your pre-requisite steps. You can retrieve this value with the following command: `$ cat /etc/letsencrypt/live/<<CIRCLECI_SERVER_DOMAIN>>/fullchain.pem`.

*Private Load Balancer (optional)* - Load balancer doesn't generate external IP addresses. 

*Replicaset (optional)* - Used to increase the amount of traffic that can be handled by the frontend. 


==== Postgres Settings

You can skip this section unless you plan on using an existing Postgres instance. By default CirecleCI server will create its own Postgres instance within the CircleCI namespace. The instance inside the CircleCI namespace will be included in the CircleCI backup and restore process. 

NOTE: If using your own PostgresSQL instance it needs to be version 12.1 or greater. 

If you select External complete the following Postgres fields: 

*PostgresSQL Service Domain  (required)* - The domain or IP address of your PostgresSQL instance. 

*PostgresSQL Service Port  (required)* - The port of your PostgresSQL instance. 

*PostgresSQL Service Username  (required)* - A user with the appropriate privileges to access your PostgresSQL instance. 

*PostgresSQL Service Password (required)* - The password of the user used to access your PostgresSQL instance. 

==== MongoDB Settings

You can skip this section unless you plan on using an existing MongoDB instance. By default CirecleCI server will create its own MongoDB instance within the CircleCI namespace. The instance inside the CircleCI namespace will be included in the CircleCI backup and restore process. 

NOTE: If using your own MongoDB instance it needs to be version 3.6 or greater. 

If you select External complete the following MongoDB fields: 

*MongoDB connection host(s) or IP(s) (required)* - 
The hostname or IP of your MongoDB instance. Specifying a port using a colon and multiple hosts for sharded instances are both supported.

*Use SSL for connection to MongoDB (required)* - 
Whether to use SSL when connecting to your external MongoDB instance

*Allow insecure TLS connections (required)* - 
If you use a self-signed certificate or one signed by a custom CA, you will need to enable this setting. However, this is an insecure setting and you should use a TLS certificate signed by a valid CA if you can.

*MongoDB user (required)* - 
The user name for the account to use. This account should have the dbAdmin role.

*MongoDB password (required)* - 
The password for the account to use.

*MongoDB authentication source database (required)* - 
The database that holds the account information, usually admin.

*MongoDB authentication mechanism (required)* - 
The authentication mechanism to use, usually SCRAM-SHA-1.

*Additional connection options (optional)* - 
Any other connection options you would like to use. This needs to be formatted as a query string (key=value pairs, separated by &, special characters need to be URL encoded). See the link:https://docs.mongodb.com/v3.6/reference/connection-string/[MongoDB docs] for available options.

==== Vault Settings

You can skip this section unless you plan on using an existing Vault instance. By default CirecleCI server will create its own Vault  instance within the cluster. The default will be included in the CircleCI backup and restore process. 

Complete the following Vault fields: 

==== Artifact and Encryption Signing Settings
Encryption and artifact signing keys were created during prerequisites. You can enter them here now. 

Complete the following fields: 

*Artifact Signing Key (required)*

*Encryption Signing Key (required)*

==== Github Settings
You created your Github OAuth application in the prerequisite steps, use the data to complete the following:

*Github Type (required)* - 
Select Cloud or Enterprise (on premise)

*OAuth Client ID (required)* - 
The OAuth Client ID provided by Github. 

*OAuth Client Secret (required)* - 
The OAuth Client Secret provided by Github. 

==== Object Storage Settings

You created your Object Storage Bucket and Keys in the prerequisite steps, use the data to complete the following:

===== S3 Compatible
You should have created your S3 Compatible bucket and optional IAM account during the prerequisite steps. 

*Storage Bucket Name (required)* -
The bucket used for server.

*Access Key ID (required)* -
Access Key ID for S3 bucket access.

*Secret Key (required)* -
Secret Key for S3 bucket access.

*AWS S3 Region (optional)* -
AWS region of bucket if your provider is AWS. S3 Endpoint is ignored if this option is set.

*S3 Endpoint (optional)* -
API endpoint of S3 storage provider. Required if your provider is not AWS. AWS S3 Region is ignored if this option is set.

*Storage Object Expiry (optional)* -
Number of days to retain your test results and artifacts. Set to 0 to disable and retain objects indefinitely.

===== Google Cloud Storage 
You should have created your Google Cloud Storage bucket and service account during the prerequisite steps. 

*Storage Bucket Name (required)* - 
The bucket used for server.

*Service Account JSON (required)* - 
A JSON format key of the Service Account to use for bucket access.

*Storage Object Expiry (optional)* - 
Number of days to retain your test results and artifacts. Set to 0 to disable and retain objects indefinitely.

==== Save and Deploy
Once you have completed the fields detailed above it's time to deploy. The deployment will install the core services and provide you an IP address for the Traefik load balancer. That IP address will be critical in setting up a DNS record and completing the first phase of the installation. 

NOTE: In this first stage we skipped a lot of fields in the config. Not to worry. We will revisit those in the next stages of installation.

==== Create DNS Entry 
Create a DNS entry for your Traefik load balancer, i.e. circleci.your.domain.com and app.circleci.your.domain.com. The DNS entry should align with the DNS names used when creating your TLS certificate and Github OAuth app during the prerequisites steps. All traffic will be routed through this DNS record. 

You will need the IP address of the Traefix load balancer. You can find it with the following terminal command:

----
kubectl get service circleci-server-traefik --namespace=nfish-circleci-server
----

For more information on adding a new DNS record, see the following documentation:

link:https://cloud.google.com/dns/docs/records#adding_a_record[Managing Records] (GCP)

link:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-creating.html[Creating records by using the Amazon Route 53 Console] (AWS)

==== Validation

You should now be able to navigate to your CircleCI server installation and login to the application successfully. Now let’s move on to build services. It may take a while for all your services to be up, you can periodically check by running the following command. You are looking for the “frontend” pod to be status of running and ready should show 1/1. 

----
kubectl get pods -n <<circleci installation namespace>>
----

## Phase 3: Build Services Installation

==== Output Processor 
===== Overview 
Output process or is responsible for handling the output from nomad clients. It is a key service to scale if you find your system slowing down. We recommend increasing the output processor replica set to scale the service up to meet demand. 

Enter the following in Settings: 

*Output Processor Load Balancer (required)* - 
The following command will provide the IP address of the service `kubectl get service output-processor --namespace=circleci-server`

*Save your configuration*. You will deploy and validate your setup after you complete Nomad client setup.  

==== Nomad Clients 
===== Overview  
As mentioned in the link:https://circleci.com/docs/2.0/server-3-overview[Overview], Nomad is a workload orchestration tool that CircleCI uses to schedule (via Nomad server) and run (via Nomad Clients) CircleCI jobs.

Nomad clients are installed outside of the Kubernetes cluster, while their control plane (Nomad server) is installed within the cluster. The communication between your Nomad clients and the nomad control plane are secured with mTLS. The mTLS certificate, private key, and certificate authority will be output after you complete the Nomad clients installation. 

Once completed you will be able to update your CircleCI server configuration so your Nomad control plane is able to communicate with your Nomad clients. 

===== Cluster Creation with Terraform

CircleCI curates Terraform modules to help install Nomad clients in your cloud provider of choice. You can browse the modules in our link:https://circleci.com/docs/2.0/server-3-overview[public repository]including example Terraform config files (man.tf) for both AWS and GKEs for main.tf. Some information about your cluster and server installation is required to complete your main.tf. How to get this information is described in the following sections.

===== AWS
You will need some information about your cluster and server installation to complete the required fields for the terraform configuration file (main.tf). A full example as well as a full list of variables can be found link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[here]. 

*Server_endpoint* - You will need to know the Nomad Server endpoint, which is the external IP address of the nomad-server-external Loadbalancer. You can get this information with the following command: 

----
kubectl get service nomad-server-external --namespace=circleci-server 
----

*Subnet ID (subnet)*, *VPC ID (vpcId)*, and *DNS server (dns_server)* of your cluster. 
Run the following command to get the cluster VPC ID (vpcId), CIDR block (serviceIpv4Cidr), and subnets (subnetIds): 

----
aws eks describe-cluster --name=<<name of your cluster>>
----

This will return something similar to the following: 

[source, json]
{...
"resourcesVpcConfig": {
    "subnetIds": [
        "subnet-033a9fb4be69",
        "subnet-04e89f9eef89",
        "subnet-02907d9f35dd",
        "subnet-0fbc63006c5f",
        "subnet-0d683b6f6ba8",
        "subnet-079d0ca04301"
    ],
    "clusterSecurityGroupId": "sg-022c1b544e574",
    "vpcId": "vpc-02fdfff4c",
    "endpointPublicAccess": true,
    "endpointPrivateAccess": false
...
"kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16"
        },
...
}

Then, using the VPCID you just found, run the following command to get the Cidr Block for your cluster. For AWS, the DNS Server is the third IP in your CIDR block (serviceIpv4Cidr), for example your CIDR block might be 10.100.0.0/16 so the third IP would be 10.100.0.2.

----
aws ec2 describe-vpcs --filters Name=vpc-id,Values=<<vpcId>>
----

This will return something like the following: 

[source, json]
{...
"CidrBlock": "192.168.0.0/16",
"DhcpOptionsId": "dopt-9cff",
"State": "available",
"VpcId": "vpc-02fdfff4c"
...}


Once you have filled in the appropriate information you can deploy your nomad clients by running the following from within the directory of the main.tf file. 

----
terraform init
----
----
terraform plan
----
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it will output the certificates and keys needed for configuring the Nomad control plane in CircleCI server. Make sure to copy them somewhere safe. The apply process usually only takes a minute. 

===== GKE 
You will need the IP address of the Nomad control plane (Nomad Server), which was created when you deployed CircleCI Server. You can get the IP address by issuing the following command: 

----
kubectl get service nomad-server-external --namespace=circleci-server 
----

You will also need the following information: 

* The GPC Project you want to run nomad clients in. 
* The GPC Zone you want to run nomad clients in. 
* The GPC Region you want to run nomad clients in. 
* The GPC Network you want to run nomad clients in. 
* The ID of the GPC subnet you want to run nomad clients in. 

You can copy the following example to your local environment and fill in the appropriate information for your specific setup. Once you have filled in the appropriate information you can deploy your nomad clients by running. 

----
terraform init
----
----
terraform plan
----
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it will output the certificates and key needed for configuring the Nomad control plane in CircleCI server. Make sure to copy them somewhere safe.

==== Configure and Deploy
Now that you have successfully deployed your Nomad clients, you can configure CircleCI Server and the Nomad control plane. Log in to the KOTs admin console and navigate to your current config. 

Enter the following in Settings: 

*Nomad Load Balancer (required)* - 
kubectl get service nomad-server-external --namespace=circleci-server

*Nomad Server Certificate (required)* - 
Provided in the output from the terraform apply

*Nomad Server Private Key (required)* - 
Provided in the output from the terraform apply

*Nomad Server Certificate Authority (CA) Certificate (required)* - 
Provided in the output from the terraform apply

*Output Processor Load Balancer (required)* - 
kubectl get service output-processor --namespace=circleci-server

Click the *Save config* button to update your installation and re-deploy server.

==== Nomad Clients Validation

CircleCI has created a project called RealityChecker which allows you to test your Server installation. We are going to follow the project so we can verify that the system is working as expected. As you continue through the next phase sections of realitychecker will move from red to green. 

To run realitycheck you will need to clone the repository depending on your Github setup you can do one of the following. 

===== Github Cloud 
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
----

===== Github Enterprise
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
git remote set-url origin <your-ghe-repo-url>
git push
----

Once you have successfully cloned the repository you can follow it from within your CircleCI server installation. You will need to set the following variables. For full instructions please see the repository readme. 

Environmental Variables
[options="header,footer"]
|=======================
|Name|Value
|CIRCLE_HOSTNAME|<<your circle ci installation URL>>
|CIRCLE_TOKEN|<<your circle ci api token>>
|=======================



Contexts
[options="header,footer"]
|======================= 
|Name| Environmental Variable Key|Environmental Variable Value
|org-global| CONTEXT_END_TO_END_TEST_VAR| Leave blank
|individual-local| MULTI_CONTEXT_END_TO_END_VAR| Leave blank
|=======================

Once you have configured the environmental variables and contexts rerun the tests. You should see the features and resource jobs complete successfully. Your test results should look something like the following. 

==== VM Service

VM Service configures VM and remote docker jobs. You can configure a number of options for VM service, such as scaling rules. VM service is unique to EKS and GKE installations, because it specifically relies on features of these cloud providers.

===== EKS
*Step 1*: Get the Information Needed to Create Security Groups 

The following will return your VPC ID (vpcId), CIDR Block (serviceIpv4Cidr), Cluster Security Group ID (clusterSecurityGroupId) and Cluster ARN (arn) values, you will need these throughout this section: 

----
aws eks describe-cluster --name=<<your cluster name>>
----

*Step 2*: Create Security Group

Run the following commands to create a security group for VM service. 

----
aws ec2 create-security-group --vpc-id "<<vpcId>>" --description "CircleCI VM Service security group" --group-name "circleci-vm-service-sg"
----

This will output a GroupID to be used in the following commands: 

[source, json]
{
    "GroupId": "sg-0cd93e7b30608b4fc"
}

*Step 3*: Apply Security Group Nomad

Use the created security group and cidr block values to apply the security group to the following: 

----
aws ec2 authorize-security-group-ingress --group-id "<<GroupId>>" --protocol tcp --port 22 --cidr "<<serviceIpv4Cidr>>"
----
----
aws ec2 authorize-security-group-ingress --group-id "<<GroupId>>" --protocol tcp --port 2376 --cidr "<<serviceIpv4Cidr>>"
----

NOTE: If you created your Nomad Clients in a different subnet than the circleci server then you would need to rerun the above two commands with each subnet CIDR. 

*Step 4*: Apply the Security Group for SSH

Run the following command to apply the security group rules so users can SSH into jobs:

----
aws ec2 authorize-security-group-ingress --group-id "<<GroupId>>" --protocol tcp --port 54782
----

*Step 5*: Create User

Create a new user with programmatic access. 
----
aws iam create-user --user-name circleci-server-vm-service
----

*Step 6*: Create Policy 

Create a policy.json file with the following content. You should fill in Cluster Security Group ID (clusterSecurityGroupId) and Cluster ARN (arn) below. 

[source, json]
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<<clusterSecurityGroupID>>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-server-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-server-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-server-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<<arn>>"
        }
      }
    }
  ]
}

*Step 7*: Attach Policy to User 

Once you have created the policy.json file attach it to an IAM policy and created user. 

----
aws iam put-user-policy --user-name circleci-server-vm-service --policy-name circleci-server-vm-service --policy-document file://policy.json
----

*Step 8*: Create an access key and secret for the user

If you haven’t already you will need an access key and secret for the circleci-server-vm-service user. You can create that by running the following command. 

----
aws iam create-access-key --user-name circleci-server-vm-service
----

*Step 9*: Configure Server

Configure VM Service through the KOTs admin console. The following fields need to be completed for VM service to operate properly. 

*AWS Region (required)* - 
This is the region the application is in.

*Subnet ID (required)* - 
Choose a subnet (public or private) where the VMs should be deployed. If you haven’t created a unique subnet you can use the subnet of the cluster. 

*Security Group ID (required)* - 
This is the security group that will be attached to the VMs. It was created previously. 

*AWS IAM Access Key ID (required)* -
AWS Access Key ID for EC2 access.

*AWS IAM Secret Key (required)* -
IAM Secret Key for EC2 access.

*AWS Windows AMI ID (optional)* -
If you require Windows builders, you can supply an AMI ID for them here.

Once you have configured the fields, *save your config* and deploy your updated application. 

==== GKE 

You will need additional information about your cluster to complete the next section. 
----
gcloud container clusters describe
----

Which will return something like the following which will include network, region and other details that you will need to complete the next section: 

[source, json]
addonsConfig:
  gcePersistentDiskCsiDriverConfig:
    enabled: true
  kubernetesDashboard:
    disabled: true
  networkPolicyConfig:
    disabled: true
clusterIpv4Cidr: 10.100.0.0/14
createTime: '2021-08-20T21:46:18+00:00'
currentMasterVersion: 1.20.8-gke.900
currentNodeCount: 3
currentNodeVersion: 1.20.8-gke.900
databaseEncryption:
…

*Step 1*: Create Firewall Rules

Run the following commands to create a firewall rules for VM service in GKE.

----
$ gcloud compute firewall-rules create "circleci-vm-service-internal-nomad-fw" --network "<<network>>" --action allow --source-ranges "0.0.0.0/0" --rules "TCP:22,TCP:2376"
----

NOTE: You can find the Nomad clients CIDR based on the region by referring to the table here if you have used auto-mode. 

----
$ gcloud compute firewall-rules create "circleci-vm-service-internal-k8s-fw" --network "<<network>>" --action allow --source-ranges "<<clusterIpv4Cidr>>" --rules "TCP:22,TCP:2376"
----
----
$ gcloud compute firewall-rules create "circleci-vm-service-external-fw" --network "<<network>>" --action allow --rules "TCP:54782"
----

*Step 2*: Create User

We recommend you create a unique service account used exclusively by VM Service. The Compute Instance Admin (Beta) role is broad enough to allow VM Service to operate. If you wish to make permissions more granular, you can use the Compute Instance Admin (beta) role documentation as reference.

----
gcloud iam service-accounts create circleci-server-vm --display-name "circleci-server-vm service account"
----

*Step 3*: Get the service account email address 

----
gcloud iam service-accounts list --filter="displayName:circleci-server-vm service account" --format 'value(email)'
----

*Step 4*: Apply role to service account 

Apply the Compute Instance Admin (Beta) role to the service account. 

----
gcloud projects add-iam-policy-binding <<RROJECT_ID>> --member serviceAccount:<<SERVICE_ACCOUNT_EMAIL>> --role roles/compute.instanceAdmin --condition=None
----

And 

----
gcloud projects add-iam-policy-binding <<PROJECT_ID>> --member serviceAccount:<<SERVICE_ACCOUNT_EMAIL>> --role roles/iam.serviceAccountUser --condition=None
----

*Step 5*: Get JSON Key File

After running the following, you should have a file named circleci-server-vm-keyfile in your local working directory. You will need this when you configure your server installation. 

----
gcloud iam service-accounts keys create circleci-server-vm-keyfile --iam-account <<SERVICE_ACCOUNT_EMAIL>>
----

*Step 6*: Configure Server

Configure VM Service through the KOTs admin console. 

*VM Service Load Balancer (required)*
This can be found using the following command:

kubectl get service vm-service --namespace=circleci-server

*GCP project ID (required)* - 
Name of the GCP project the cluster resides.

*GCP Zone (required)* - 
GCP zone the virtual machines instances should be created in for example “us-east1-b”.

*GCP VPC Network (required)* - 
Name of the VPC Network.

*GCP VPC Subnet (optional)* - 
Name of the VPC Subnet. If using auto-subnetting, leave this field blank.

*GCP Service Account JSON Key File (required)* - 
Copy and paste the contents of your service account JSON file.

*GCP Windows Image (optional)* - 
Name of the image used for Windows builds. Leave this field blank if you do not require them.

Click the *Save config* button to update your installation and re-deploy server.

===== Additional VM Service Configuration

*Number of <VM type> VMs to keep prescaled (optional)* - By default, this field is set to 0 which will create and provision instances of a resource type on demand. You have the option of preallocating up to 5 instances per resource type. Preallocating instances lowers the start time allowing for faster machine and remote_docker builds. Note, that preallocated instances are always running and could potentially increase costs. Decreasing this number may also take up to 24 hours for changes to take effect. You have the option of terminating those instances manually, if required.

==== VM Service Validation

Once you have configured and deployed CircleCI server you should validate that VM Service is operational. You can re-run the reality checker project within your CircleCI installation and you should see the VM Service Jobs complete with green. At this point all tests should pass with green. 

## Runners 

==== Overview 

Runners do not require any additional server configuration. Server ships ready to work with Runners. However, you do need to create runner and configure the runner agent to be aware your server installation. For complete instructions to setup runner please see the link:https://circleci.com/docs/2.0/runner-overview/?section=executors-and-images[runner documentation]. 

NOTE: Runner requires a namespace per organization. Server can have many organizations. If your company has multiple organizations within your CircleCI installation you will need to setup runner namespace for each organization within your server installation. 

## Phase 4: Post Installation

==== Orbs 

Server installations include their own local orb registry. This registry is private to the server installation. All orbs referenced in configs reference the orbs in the server orb registry. You are responsible for maintaining orbs; this includes copying orbs from the public registry, updating orbs that may have been copied prior, and registering your companies private orbs if they exist.

===== Managing Orbs 

Orbs are accessed via the CircleCI CLI. Orbs require your CircleCI user to be an admin. They also require a personal link:https://circleci.com/docs/2.0/managing-api-tokens/[api token]. Providing a local repository location using the --host option allows you to access your local server orbs vs the public cloud orbs. For example, if your server installation is located at http://circleci.somehostname.com, then you can run orb commands local to that orb repository by passing '--host http://cirlceci.somehostname.com'.

===== List Available Orbs 

To list available public orbs, visit the orb directory or run:
----
circleci orb list
----

To list available private orbs (registered in your local server orb repository) run:
----
circleci orb list --host <your server install domain> --token <your api token>
----

===== Import Public Orb

To import a public orb to your local server orb repository:
----
circleci admin import-orb ns[/orb[@version]] --host <your server installation domain> --token <your api token>
----

===== Fetch Public Orb Update 

To update a public orb in your local server orb repository with a new version, run:
----
circleci admin import-orb ns[/orb[@version]] --host <your server installation domain> --token <your api token>
----

For more Orb information, please refer to the Orb docs for the cloud product.


==== Email Notifications 

Build notifications are sent via email.

*Email from address (required)* - The from address for the email.  

*Email Submission server hostname (required)* - Host name of the submission server (e.g., for Sendgrid use smtp.sendgrid.net).

*Username (required)* - Username to authenticate to submission server. This is commonly the same as the user’s e-mail address.

*Password (required)* - Password to authenticate to submission server.

*Port (optional)* - Port of the submission server. This is usually either 25 or 587. While port 465 is also commonly used for email submission, it is often used with implicit TLS instead of StartTLS. Server only supports StartTLS for encrypted submission. 

NOTE: Outbound connections on port 25 are blocked on most cloud providers. Should you select this port, be aware that your notifications may fail to send
Enable StartTLS: Enabling this will encrypt mail submission. 

NOTE: You should only disable this if you can otherwise guarantee the confidentiality of traffic

Click the *Save config* button to update your installation and re-deploy server.

## What to read next

* https://circleci.com/docs/2.0/server-3-install-hardening-your-cluster[Hardening Your Cluster]
* https://circleci.com/docs/2.0/server-3-install-migration[Server 3.x Migration]