---
version:
- Cloud
- Server
---
= Fail Tests Faster - Open Preview
:page-layout: classic-docs
:page-liquid:
:page-description: A new way to tighten your feedback loop when running tests on CircleCI
:icons: font
:toc: macro
:toc-title:


WARNING: warning banner
Preview, no guarantees this works.  See Known Limitations as well.

[#Motivation]
== Motivation

Feedback loops: we want them to be as tight as possible.  Wasting credits, not good!

“I don't have to spend 30 minutes [waiting] if I have a test that fails in the first 30 seconds. It's not just extra wait time, but also extra credits.”

Failing-fast is not a novel concept, most test runners already support some version of it (ie. jest, pytest).  Two main differentiators for CircleCI though:

When used with test splitting to stop all parallel runs (tasks)
Today (without fail-fast), there is no way to tell a separate parallel run (task) in a CircleCI job to stop executing because one task found a test failure

[#use-cases]
=== Use Cases
- Test jobs with flaky tests (from research)
- Browser testing tends to be very flaky
- Test jobs that take longer than 7 minutes (from research, 7 minutes = approximation for “takes a long time”)
- Users who leverage self-hosted runners && want to minimize the cost of their infrastructure 
- Test suites with interdependent tests ie. some E2E tests
- When adding new tests as part of adding new functionality to a project ie. on a feature branch (from Discuss)
- Optimizing credit consumption
- A user orders tests to execute the faster tests first, then the slower ones. If the faster ones fail, don’t run the slower ones to save time/credits (from Discuss)
- If static analysis fails, don’t run the remaining tests (from Discuss)

[#quick-start]
== Quick-start

[#before]
=== Before

```yml
 - run:
         name: Run tests
         command: |
           mkdir test-results
           pytest --junitxml=test-results/junit.xml
```

[#after]
=== After

```yml
jobs:
 build-and-test:
   docker:
     - image: cimg/python:3.11.0
steps:
     - checkout
     - python/install-packages:
         pkg-manager: pip
 
- run:
         name: Run tests
         command: |
           mkdir test-results
           pytest --collect-only -q | grep -e "\.py" | circleci tests run  --command="xargs pytest --junitxml=test-results/junit.xml -v --" --fail-fast --batch-count=3 --verbose --test-results-path="test-results"
     - store_test_results:
         path: test-results
```
[#important-pieces]
==== Important Pieces

* `pytest --collect-only -q | grep -e “\.py” |`
  ** This piece of the command is very similar to a link:https://circleci.com/docs/troubleshoot-test-splitting/#video-troubleshooting-globbing[glob command] used for link:https://circleci.com/docs/test-splitting-tutorial/[CircleCI’s test splitting].  It provides the list of tests to the `circleci tests run` command as standard input (link:https://www.computerhope.com/jargon/s/stdin.htm[stdin]).  The tests in this example are all in one file for simplicity.  This lets `--collect-only -q` only output the test names themselves.  `grep` then can get all items that end in `.py`.  If you have multiple test *files*, you can instead pass filenames with a command like: `pytest --collect-only -qq | cut -d ':' -f 1 |`
  ** A glob command is also suitable and the most common approach on CircleCI
* `circleci tests run` 
  ** This invokes the *fail-fast* functionality
  ** `--command=”xargs pytest --junitxml=tests-results/junit.xml -v --”`
   *** This calls `pytest` to run the tests and tells it to output the results in link:https://www.ibm.com/docs/en/developer-for-zos/14.1?topic=formats-junit-xml-format[JUnit format] so that the results can be uploaded to CircleCI.  **The `xargs` flag is critical to include as it tells pytest to accept a list of tests from stdin.**
   *** `-v` is used to add verbosity in the `pytest` output, not required
   *** '--' is used by `xargs` to help read from stdin, not required
  ** `--fail-fast`
   *** Tells CircleCI to fail-fast when it encounters a failure within a batch (batching explained below)
  ** `--batch-count=3'
   *** A mechanism to divide tests into groups where if one group finds a failed test when executing, subsequent groups will not execute, therefore *failing fast*. See the batching section below for details. A basic heuristic for choosing a batch count:
    **** If your tests have heavy “pre-initialization logic” (like spinning up a browser/UI or creating and configuring databases), use a small batch count like 2 or 3.  
    **** Else: use a higher number, 5-7
  ** `--verbose`
   *** Optional, adds verbosity in output. Recommended.
  ** –-test-results-path-”test-results”
   *** Optional, but link:https://circleci.com/docs/collect-test-data/[best practice for running tests on CircleCI].  Needed in conjunction with the `store_test_results` command below to enable view rich test results.

When you put all the pieces together, ...  

[#verifying-the-configuration]
==== Verifying the configuration

If the `--verbose` setting is enabled, you should see in your step output a description of the number of batches processed.  This is an indication that the job has been configured successfully.

[#batching]
== Batching

Batches are tests that have been divided that report status after completion. If the test runner command that is executed as part of that batch returns a failure, the batch will return a failed status to CircleCI and when fail-fast is enabled, halt any additional batches from kicking off. Batch counts are set to 1 by default.

If no test splitting is enabled, batches execute sequentially like the diagram shows below.

image::batching_without_test_splitting.jpg[Without Test Splitting]

If test splitting is enabled, see the diagram below.  Each task splits the tests that it’s in charge of in batches that are executed sequentially within that task.  Assume that there are 9 tests in total and the tests are split by filename.  Each task gets 3 tests.  Each batch within the 3 tasks contains 1 test.

image::batching_with_test_splitting.jpg[With Test Splitting]

After each batch within a task finishes executing its test, it checks with CircleCI to see if it should keep going to the next batch. For example, if batch 1 in task 0 immediately fails its test, it will report that failure to CircleCI.  After batch 1 from task 1 finishes executing, task 1 will check to see if it should go on to batch 2.  Because there’s already been a failure, batch 2 will not execute and the job will terminate.

[#known-limitations]
== Known Limitations

- You will only get the test results in the CCI UI for the last batch that executed.  This is in the process of being resolved.  This also means that test splitting by timing may not be perfect until this is resolved.  This will be resolved during the Preview phase.
- If you’re running code coverage as part of your testing job, using this new functionality may cause code coverage reports to be off…we’re still looking into this


[#faqs]
== FAQs

* Are batching and parallelism the same thing?
 ** *Answer:* No, see “how it works” section.  
* What happens if I already have a fail-fast setting at the test runner enabled? 
 ** *Answer:* The test runner will still honor whatever settings you give it, including options like jest’s bail.  CircleCI is simply calling the test runner command with the settings you give it but only passing in certain tests/filenames/packages on each call. So on each call, whatever settings you pass to the test runner will be honored and if fail-fast is enabled, if the test runner returns a non-zero exit code, it will look the same as if a test failed and no “bail’ was enabled.
* How does it work with orbs like Cypress orb?
 ** *Answer:* TBD
* 




//format all links to other docs pages and other websites like this for now
link:https://bing.com[bing]



image::slack-orb-create-app.png[Image title]
