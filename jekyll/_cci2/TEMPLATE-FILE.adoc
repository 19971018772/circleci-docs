---
contentTags:
  platform:
  - Cloud
  - Server v4.x
  - Server v3.x
---
= <<Re-run failed tests only>>
:page-layout: classic-docs
:page-liquid:
:page-description: <<Functionality to optimize running tests on CircleCI>>
:icons: font
:toc: macro
:toc-title:

[#motivation-and-introduction]
== Motivation and Introduction
Historically, when your workflow has flaky tests, the only option to get to a passing workflow was to [https://support.circleci.com/hc/en-us/articles/360050303671-How-To-Rerun-a-Workflow](re-run your workflow from failed).  This re-run executes *all tests*, including the tests that passed which prolongs time-to-feedback and wastes credits.

CircleCI is releasing new preview functionality to **re-run failed tests only** instead of the entire test suite when a transient test failure arises.  This re-runs failed tests from the same commit, not new ones.

[#prerequisites]
== Prerequisites

* Testing job uploads test results to CircleCI.  `filename` or `classname` attributes **must be present** in the JUnit XML output
* Testing job uses `circleci tests run` (see below for details) to execute tests

[#quick-start]
== Quick-start

[#example-config-file-before]
=== Before: Example .circleci/config.yml file

```yaml
 - run:
    name: Run tests
    command: |
      mkdir test-results
      TEST_FILES=$(circleci tests glob "**/test_*.py" | circleci tests split --split-by=timings) \
      pytest --junitxml=test-results/junit.xml $TEST_FILES
      
- store_test_results
    path: test-results
```

A snippet of a basic CircleCI configuration file that splits tests by previous timing results (xref:test-splitting-tutorial#[Intelligent test splitting]), executes Python tests, stores the test results in a new directory called `test-results`, and uploads those test results to CircleCI.  **Note:** `-o junit_family=legacy` is present to ensure that the test results being generated contain the `filename` attribute.  Not present in this snippet is the key to set `parallelism`(xref:parallelism-faster-jobs#[tutorial]).

[#example-config-file-after]
=== After: Example .circleci/config.yml file

```yaml
 - run:
    name: Run tests
    command: |
      mkdir test-results
      TEST_FILES=$(circleci tests glob "**/test_*.py")
      echo $TEST_FILES | circleci tests run --command="xargs pytest -o junit_family=legacy --junitxml=test-results/junit.xml" \
      --verbose \
      --split-by=timings

 - store_test_results
    path: test-results
```

[#breakdown-the-configuration]
==== Breakdown of the configuration

* `TEST_FILES=$(circleci tests glob "**/test_*.py")`
  ** This command uses CircleCI's xref:troubleshoot-test-splitting#video-troubleshooting-globbing[glob command] to provide a list of test files to the `circleci tests run` command as standard input (link:https://www.computerhope.com/jargon/s/stdin.htm[stdin]).  In this case, we're looking for any test file that starts with "test_" and ends with ".py".
  
* `echo $TEST_FILES |`
  ** Pass the list of test files to the `circleci tests run` command via stdin.

* `circleci tests run --command="xargs pytest -o junit_family=legacy --junitxml=test-results/junit.xml" --verbose --split-by=timings`
  ** Invoke `circleci tests run` and specify the original command used to run tests as part of the `--command=` parameter
  ** `--verbose` is an optional parameter but enables debugging messages
  ** `--split-by-timings` is an optional parameter that accomplishes intelligent test splitting by timing 
  
[#verify-the-configuration]
==== Verify the configuration

After set-up, the next time that you encounter a transient test failure, click the "Re-run failed tests only" button shown below.  

**Insert screenshot**

If the `--verbose` setting is enabled, you should see output similar to the following the next time you run this job on CircleCI:

```bash
Installing circleci-tests-plugin-cli plugin.
circleci-tests-plugin-cli plugin Installed. Version: 1.0.3349-470dac4
DEBU[2023-04-10T22:36:53Z] Attempting to read from stdin. This will hang if no input is provided. 
INFO[2023-04-10T22:36:53Z] starting batch execution                      batch_count=1 batches_processed=0 total_batches_for_job=3
DEBU[2023-04-10T22:36:53Z] received test names: test_api.py```
```

The job should only re-run tests that are from a `classname` of `filename` that had at least one test failure.  

[#use-cases]
== Use Cases

* List item 1
* List item 2

[#additional-examples]
== Additional examples

**Configure a job running Ruby (rspec) tests to use "Re-run failed tests only"**:

First, add the following gem to your Gemfile:

```bash
gem 'rspec_junit_formatter'
```

Then, modify your test command to use `circleci tests run`:

```yaml
    - run: mkdir ~/rspec
    - run:
      command: |
        circleci tests glob spec/**/*_spec.rb | --command="xargs bundle exec rspec --format progress --format RspecJunitFormatter -o ~/rspec/rspec.xml"
 ```

Update the glob command to match your use case.  See xref:collect-test-data/#rspec#[rspec] for details on how to output test results in an acceptable format for `rspec`.

**Configure a job running Ruby (Cucumber) tests to use "Re-run failed tests only"**:

Modify your test command to look like something similar to:

```yaml
- run: mkdir -p ~/cucumber
- run:
    command: |
    circleci tests glob features/**/*.feature | --command="xargs bundle exec cucumber --format junit --out ~/cucumber/junit.xml"
 ```

Update the glob command to match your use case.  See xref:collect-test-data/#cucumber#[cucumber] for details on how to output test results in an acceptable format for `Cucumber`.

**Configure a job running Cypress tests to use "Re-run failed tests only"**:

1. Install dependencies `cypress-multi-reporters` and `mocha-junit-reporter`. If using `npm`:

```bash
npm install --save-dev cypress-multi-reporters mocha-junit-reporter
```

2. Create and setup reporter config file if it doesn't already exist, this example will call it `reporter-config.json`.

```bash
{
  "reporterEnabled": "spec, mocha-junit-reporter", // set the reporters
  "reporterOptions": {
    "mochaFile": "results/junit/junit-[hash].xml", // each suite produces its own junit :(, save them with unique hash
   }
}
```


3. Modify your test command to use the two reporter flags and `circleci tests run`:

```yaml
     -run:
        name: run tests
        command: | 
          cd ./cypress 
          npm ci 
          npm run start &
          circleci tests glob "cypress/**/*.cy.js" | circleci tests run --command="xargs npx cypress run --reporter cypress-multi-reporters --reporter-options configFile=reporter-config.json --spec"
 ```

4. Because Cypress does not output the expected `filename` attribute on its JUnit XML files, follow the steps outlined (https://github.com/michaelleeallen/mocha-junit-reporter/issues/132)[here] to massage the test results into the proper format.  In this case, we've saved a copy of the script to a file called `fix-junit.js`. You can then invoke this script by adding a new command (in addition to the command that uploads test results, `store_test_results`):

```yaml
    - run:
       when: always
       name: process test results (add in file path in junit)
       command: |
          cd ./cypress
          node ./scripts/fix-junit.js
     - store_test_results: 
       path: ./cypress/results
```  

**Configure a job running Javascript/Typescript (Jest) tests to use "Re-run failed tests only"**:

Modify your test command to look like something similar to:

```yaml
- run:
    command: |
    npx jest --listTests | circleci tests run --command="JEST_UNIT_ADD_FILE_ATTRIBUTE=true xargs npx jest --config jest.config.js --runInBand --"
    environment:
        JEST_JUNIT_OUTPUT_DIR: ./reports/
  - store_test_results:
      path: ./reports/
 ```

Update the `npx jest --listTests` command to match your use case.  See xref:collect-test-data/#jest#[jest] for details on how to output test results in an acceptable format for `jest`.

[#known-limitations]
== Known limitations

* When re-running only the failed tests, the next time that job runs, test splitting by timing may not be as efficient as it was before as the test results being stored are only from the subset of failed tests that were run
* Orbs that run tests *may* not work with this new fucntionality to start
* If a shell script is invoked to run tests, `circleci tests run` should be placed in the *shell script* itself, not `.circleci/config.yml`
* Jobs that are older than the xref:persist-data/#custom-storage-usage[retention period] for Workspaces for the organization cannot be re-run with "Re-run failed tests only"

[#FAQs]
== FAQs

I have a question or issue, where do I go?

*Answer*: Insert Discuss post.

Will this functionality re-run individual tests?

*Answer*: No, it will re-run failed test `classnames` or `filenames` that had at least 1 individual test failure

What happens if I try to use the functionality and it hasn't been set-up in my `.circleci/config.yml` file?

*Answer*: The job will fail.

When can I click the option to "Re-run failed tests only?"

*Answer*: Right now, the option will be present anytime "Re-run workflow from failed" option is present and vice versa.

I don't see my test framework on this page, can I still use the functionality

*Answer*: Yes, as long as your job meets the prerequisites enumerated at the top of this document the functionality is test runner and test framework agnostic.  You can use xref:collect-test-data/#[Collect test data] to ensure that the job is uploading test results.  Note that `classname` and `filename` is not always present by default, it may require additional configuration.  From there, follow the "Quick-start" section to modify your test command to use `circleci tests run`.  If you run into issues, comment on this Discuss post (needs link).
