---

version:
- Server v4.x
- サーバー管理者
---
= クラスタのハードニング
:page-layout: classic-docs
:page-liquid:
:page-description: このセクションでは、Kubernetes クラスタのハードニングに関する補足情報を紹介します。
:icons: font
:toc: macro

:toc-title:

このセクションでは、Kubernetes クラスタのハードニングに関する補足情報を紹介します。

toc::[]

[#network-topology]
== ネットワークトポロジ

CircleCI Server システムでは主に、Kubernetes ノード、Nomad クライアント、外部 VM という 3 種類のコンピューティングインスタンスを実行します。

これらのインスタンスは、CIDR ブロックの異なるサブネットに分けてデプロイすることを強くお勧めします。 そうすることで、システムのコンポーネント間を流れるトラフィックの制御やそれぞれの分離がしやすくなります。

リソースはできる限り非公開にすることがベストプラクティスです。 CircleCI Server システムへのユーザーアクセスが VPN 経由であり、かつ NAT ゲートウェイが適切に設定されていれば、パブリック IP アドレスの割り当ては必要ありません。 そうでない場合は、少なくとも `circleci-proxy` ロード バランサー用にパブリックサブネットを 1 つ用意する必要があります。

ただし、この場合には、パブリックサブネットに Nomad クライアントと VM を配置して、ジョブにユーザーが SSH 接続できるようにするとともに、ネットワークルールでアクセススコープを設定することもお勧めします。

現時点では、GCP はカスタムのサブネット化には対応していません。 カスタムのサブネット化は、今後の更新/リリースで使用可能になる予定です。

NOTE: NGINX リバースプロキシは、 https://github.com/Kong/charts[Kong] の前に配置され、 `circleci-proxy` という名前の Kubernetes サービスとして公開されます。 NGINX はトラフィックを `kong` 、 `vm-service` 、 `output-processor` 、 `nomad` にルーティングします。

CAUTION: Amazon Certificate Manager (ACM) をご使用の場合は、NGINX のサービスは `circleci-proxy` ではなく `circleci-proxy-acm` になります。 TLS 証明書の処理を他の方法から ACM に変更した場合、ロードバランサーが再作成され、お客様の <domain> とアプリの <domain> の関連付けられている DNS レコードを再ルーティングする必要があります。

[#network-traffic]
== ネットワークトラフィック

このセクションでは、CircleCI Server システムの稼働に必要な最小要件について説明します。 ワークロードによっては、Nomad クライアントと VM の送信にルールを追加する必要があります。 クラウドプロバイダーごとに名称は異なります。 そのため、ファイアウォール ルールやセキュリティグループなどを使用した実装が必要になる可能性があります。

表中の "外部" は、特に断りがない限り、すべての外部 IPv4 アドレスを意味します。 ただし、外部トラフィックすべてにプロキシを使用している場合など、お使いの環境によっては、より具体的に指定することができます。

また、下記のルールは、特別な記載のない限り、TCP 接続に対してステートフルで設定するものとします。 ステートレスルールを使用する場合には、下表の各項目に該当する送受信ルールをそれぞれ作成してください。

[#reverse-proxy-status]
=== リバースプロキシのステータス

CircleCI Server でサービスのルーティングトラフィックステータスを確認し、問題がある場合は警告したい場合があります。 CircleCI Server では NGINX と Kong の両方を使用しているため、両方のステータスページをポート 80 経由で公開しています。

[.table.table-striped]
[cols=2*, options="header", stripes=even]
|===
|サービス
|エンドポイント

|NGINX
|`/nginx_status`

|Kong
|`/kong_status`
|===

[#kubernetes-load-balancers]
== Kubernetes ロードバランサー

お使いの環境によっては、ロードバランサーが透過的である (ネットワークトポロジの個別のレイヤーとしては扱われない) ことがあります。 この場合には、ロードバランサーの背後にあるネットワークトラフィックの送信先またはソースに対して、本セクションのルールを直接適用してください。 CircleCI Server システムで使用している負荷分散の種類に応じたネットワークセキュリティルールの適切な適用方法については、ご利用のクラウドプロバイダーのドキュメントをご覧ください。

[#ingress-load-balancers]
=== 受信

ロードバランサーのトラフィックルールが自動的に作成されていない場合は、次の各ポートを設定してください。

[.table.table-striped]
[cols=4*, options="header", stripes=even]
|===
|名前
|ポート
|ソース
|目的

|circleci-proxy/-acm
|80
|外部
|ユーザーインターフェースとフロントエンド API

|circleci-proxy/-acm
|443
|外部
|ユーザーインターフェースとフロントエンド API

|circleci-proxy/-acm
|3000
|Nomad クライアント
|Nomad クライアントとの通信

|circleci-proxy/-acm
|4647
|Nomad クライアント
|Nomad クライアントとの通信

|circleci-proxy/-acm
|8585
|Nomad クライアント
|Nomad クライアントとの通信
|===

[#egress-load-balancers]
=== 送信

必要な送信は、Kubernetes ロードバランサーのポート (30000 ～ 32767) 上にある Kubernetes ノードへの TCP トラフィックのみです。 ただし、ロードバランサーが透過的であればこの送信は必要ありません。

[#common-rules-for-compute-instances]
== コンピューティングインスタンスの共通ルール

以下のルールは、ロードバランサーを除くすべてのコンピューティングインスタンスに適用されます。

[#ingress-common]
=== 受信

インスタンスに SSH でアクセスする必要がある場合は、該当するインスタンスへの TCP 接続用にポート 22 を開放してください。
ルールの範囲は、許可されたソースIPにできるだけ近いものにすること、または、必要な場合にのみそのようなルールを追加することをお勧めします。

[#egress-common]
=== 送信

ほとんどの環境では、インスタンスすべてに対してインターネットリソースへのアクセスを許可することになります。 これを行うには、VPC 内にある DNS サーバーへのポート 53 からの UDP と TCP の送信を許可する必要があります。また、HTTP と HTTPS それぞれのトラフィックについて、ポート 80 と 443 からの TCP の送信を許可する必要もあります。
ジョブのビルドを行うインスタンス (Nomad クライアントや外部 VM) では、多くの場合、SSH 経由でご利用中の VCS からコードをプルする必要があります (TCP ポート 22)。 SSH は、外部 VM との通信にも使用されます。 そのため、少なくとも送信先が VM サブネットおよび VCS であるインスタンスすべてについて、SSH を許可する必要があります。

[#kubernetes-nodes]
== Kubernetes ノード

[#intra-node-traffic]
=== ノード間のトラフィック

デフォルトでは、Kubernetes クラスタ内のトラフィックはネットワークポリシーにより規定されています。 つまり、Kubernetes ノード間のトラフィックを特別に制限する必要はなく、Kubernetes ノード間のトラフィックはすべて許可してかまいません。

クラスタ内でネットワーク ポリシーを使用するには、クラウド プロバイダーや環境設定にもよりますが、追加の手順を実行する必要があります。 以下の資料を参考にしてください。

* https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kuberenetes ネットワークポリシーの概要]
* https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy[Google Cloud でのクラスタネットワークポリシーの作成]
* https://docs.aws.amazon.com/eks/latest/userguide/calico.html[Amazon EKS への Calico のインストール]

[#ingress-kubernetes]
=== 受信

マネージドサービスを使用している場合は、ロードバランサーおよび許可済みのポート範囲からの送信トラフィックに対して作成されているルールを確認できます。 受信側の設定では、Kubernetes ロードバランサーの標準のポート範囲 (30000 ～ 32767) を許可するだけで十分です。 ただし、透過的なロードバランサーを使用している場合は、上記のロードバランサー用受信ルールを適用する必要があります。

[#egress-kubernetes]
=== 送信

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|送信先
|目的

|2376
|VM
|VM との通信

|4647
|Nomad クライアント
|Nomad クライアントとの通信

|すべてのトラフィック
|その他のノード
|クラスタ内トラフィックの許可
|===

[#nomad-clients-ingress-egress]
== Nomad クライアント

Nomad クライアント同士は、通信する必要はありません。 Nomad クライアント インスタンス 間のトラフィックを完全にブロックできます。

[#ingress-nomad]
=== 受信

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|ソース
|目的

|4647
|K8s ノード
|Nomad サーバーとの通信

|64535-65535
|外部
|SSH でのジョブ再実行機能
|===

[#egress-nomad]
=== 送信

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|送信先
|目的

|22
|VM
|SSH communication with VMs

|2376
|VM
|Docker communication with VMs

|3000
|VM サービスのロード バランサー
|内部通信

|4647
|Nomad のロード バランサー
|内部通信

|8585
|出力プロセッサのロード バランサー
|内部通信
|===

[#external-vms]
== 外部ポート

Nomad クライアントと同じく、外部 VM 同士も通信する必要はありません。

[#ingress-external]
=== 受信

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|ソース
|目的

|22
|Kubernetes ノード
|内部通信

|22
|Nomad クライアント
|内部通信

|2376
|Kubernetes ノード
|内部通信

|2376
|Nomad クライアント
|内部通信

|54782
|外部
|SSH でのジョブ再実行機能
|===

[#egress-external]
=== 送信

設定が必要な送信ルールは、VCS へのインターネット アクセスと SSH 接続のみです。

[#notes-on-aws-networkingl]
== Notes on AWS networking with VM service

When using the EC2 provider for VM service, there is an `assignPublicIP` option available in the `values.yaml` file.

[source,yaml]
----
vm_service:
  ...
  providers:
    ec2:
      ...
      assignPublicIP: false
----

By default this option is set to false, meaning any instance created by VM service will only be assigned a private IP address.

Communication to start a virtual machine (VM), and run a job, occurs in two stages:

. The `vm-service` pod establishes a connection to the newly created VM via ports `22` and `2376`.
. The nomad client running the job establishes a connection to the newly created VM via ports `22` and `2376`.

[#private-ips-only]
=== Private IPs only

When the `assignPublicIP` option is set to false, restricting traffic with security group rules between services can be done using the https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-security-group-ingress.html[Source Security Group ID parameter].

Within the ingress rules of the VM security group, the following rules can be created to harden your installation:

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|Origin
|用途

|22
|Nomad clients' security group
|Allows nomad clients to SSH into VM

|2376
|Nomad clients' security group
|Allows nomad clients to connect to docker on VM

|22
|EKS cluster security group
|Allows vm-service pods to SSH into VM

|2376
|EKS cluster security group
|Allows vm-service pods to connect to docker on VM

|54782
|CIDR range of your choice
|Allows users to SSH into failed vm-based jobs and to retry and debug
|===

[#using-public-ips]
=== Using Public IPs

When the `assignPublicIP` option is set to true, all EC2 instances created by VM service are assigned **public** ipv4 addresses, and, as such, all services communicating with them do so via their public addresses.

SSH traffic from the `vm-service` pod will flow through the NAT gateway of the subnet of the cluster. Since traffic moves outside the VPC it is not possible to restrict traffic by security group origin. It is instead necessary to add the IPs of the NAT gateway(s) used by the cluster to your safelist.

If both nomad clients and VM service VMs have been assigned public IPs, SSH and docker traffic will route through the subnets' internet gateways. Since traffic moves through the public internet, security groups are no longer an option for restricting traffic. In order to restrict access on these ports, the public IPv4 addresses of the nomad-clients must be added to the safelist in the VM service security group ingress rules. Keep in mind that these IPs and machines are ephemeral, and will require a mechanism to update the VM service security group on change.

When hardening an installation where the VM service uses public IPs, the following rules can be created.

[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|ポート
|Origin
|用途

|22
|Individual ipv4 addresses of all nomad clients (or 0.0.0.0/0 to allow for any possible assigned IP).
|Allows nomad clients to SSH into VM.

|2376
|Individual ipv4 addresses of all nomad clients (or 0.0.0.0/0 to allow for any possible assigned IP).
|Allows nomad clients to connect to docker on VM.

|22
|Cluster NAT gateway ipv4 ranges
|Allows traffic to the VM from the `vm-service` pods.

|2376
|Cluster NAT gateway ipv4 ranges
|Allows traffic to the VM from the `vm-service` pods.

|54782
|CIDR range of your choice
|Allows users to SSH into failed vm-based jobs to retry and debug.
|===

ifndef::pdf[]

== 次のステップ

* link:/docs/ja/server/installation/migrate-from-server-3-to-server-4[Server v3.x から Server v4.x への移行]
* link:/docs/ja/server/operator/operator-overview[Server 4.x オペレーターの概要]
+
endif::[]