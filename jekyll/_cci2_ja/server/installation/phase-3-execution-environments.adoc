---

version:
- Server v4.x
- サーバー管理者
---
= ステップ 3 - 実行環境
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro

:toc-title:

// This doc uses ifdef and ifndef directives to display or hide content specific to Google Cloud Storage (env-gcp) and AWS (env-aws). Currently, this affects only the generated PDFs. To ensure compatability with the Jekyll version, the directives test for logical opposites. For example, if the attribute is NOT env-aws, display this content. For more information, see https://docs.asciidoctor.org/asciidoc/latest/directives/ifdef-ifndef/.

Before you begin with the CircleCI server v4.x execution installation phase, ensure you have run through link:/docs/server/installation/phase-1-prerequisites[Phase 1 – Prerequisites] and link:/docs/server/installation/phase-2-core-services[Phase 2 - Core Services Installation].

////
.Installation Experience Flow Chart Phase 3
image::server-install-flow-chart-phase3.png[Flow chart showing the installation flow for server 3.x with phase 3 highlighted]
////

NOTE: 以下のセクションでは、 `< >` で示されている箇所にご自身の詳細情報を入力してください。

toc::[]

[#nomad-clients]
== 1. Nomad クライアント

Nomad is a workload orchestration tool that CircleCI uses to schedule (through Nomad server) and run (through Nomad clients) CircleCI jobs.

Nomad クライアントは Kubernetes クラスタの外部にインストールされ、コントロールプレーン（Nomad サーバー）はクラスタ内にインストールされます。 Nomad クライアントと Nomad コントロールプレーン間の通信は、 mTLS によって保護されます。 Nomad クライアントのインストールが完了すると、 mTLS 証明書、プライベートキー、および認証局が出力されます。


[#create-your-cluster-with-terraform]
=== a.  Create your cluster with Terraform

CircleCI では、任意のクラウドプロバイダーに Nomad クライアントをインストールできるように Terraform モジュールをキュレーションしています。 You can browse the modules in our link:https://github.com/CircleCI-Public/server-terraform[public repository], including example Terraform config files for both AWS and GCP.

// Don't include this section in the GCP PDF:

ifndef::env-gcp[]

[#aws-cluster]
==== AWS cluster

You need some information about your cluster and server installation to populate the required variables for the Terraform module. A full example, as well as a full list of variables, can be found in the link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[example AWS Terraform configuration].

* *Server_endpoint* - This is the domian name of the CircleCI application appended with port number `4647`.
* *Subnet ID (subnet)*, *VPC ID (vpc_id)*, and *DNS server (dns_server)* of your cluster.
Run the following command to get the cluster VPC ID (vpcId), and subnets (subnetIds):
+
[source,shell]
----
aws eks describe-cluster --name=<YOUR_CLUSTER_NAME>
----
+
すると、以下のようなコマンドが返されます。
+
[source,json]
----
{...
"resourcesVpcConfig": {
    "subnetIds": [
        "subnet-033a9fb4be69",
        "subnet-04e89f9eef89",
        "subnet-02907d9f35dd",
        "subnet-0fbc63006c5f",
        "subnet-0d683b6f6ba8",
        "subnet-079d0ca04301"
    ],
    "clusterSecurityGroupId": "sg-022c1b544e574",
    "vpcId": "vpc-02fdfff4c",
    "endpointPublicAccess": true,
    "endpointPrivateAccess": false
    }
...
"kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16"
        },
...
}
----
+
次に、見つけた VPCID を使用して次のコマンドを実行し、クラスタの CIDR ブロックを取得します。 AWS の場合、 DNS サーバーは CIDR ブロック (`CidrBlock`) の３番目の IP です。たとえば、CIDR ブロックが `10.100.0.0/16` の場合、 3 番目の IP は `10.100.0.2` になります。
+
[source,shell]
----
aws ec2 describe-vpcs --filters Name=vpc-id,Values=<YOUR_VPCID>
----
+
すると、以下のようなコマンドが返されます。
+
[source,json]
----
{...
"CidrBlock": "192.168.0.0/16",
"DhcpOptionsId": "dopt-9cff",
"State": "available",
"VpcId": "vpc-02fdfff4c"
...}
----

適切な情報を入力したら、以下を実行することにより Normad クライアントをデプロイできます。

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

Terraform は、Nomad クライアントのスピンアップが完了すると、CircleCI Server で Nomad コントロールプレーンを設定するために必要な証明書とキーを出力します。 それらを安全な場所にコピーします。 この適用プロセスには通常 1 分しかかかりません。

// Stop hiding from GCP PDF:

endif::env-gcp[]

// Don't include this section in the AWS PDF:

ifndef::env-aws[]

[#gcp-cluster]
==== GCP cluster

You need the following information:

* The Domain name of the CircleCI application
* The GCP Project you want to run Nomad clients in
* The GCP Zone you want to run Nomad clients in
* The GCP Region you want to run Nomad clients in
* The GCP Network you want to run Nomad clients in
* The GCP Subnetwork you want to run Nomad clients in

A full example, as well as a full list of variables, can be found in the link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[example GCP Terraform configuration].

適切な情報を入力したら、以下を実行することにより Normad クライアントをデプロイできます。

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

Terraform は、Nomad クライアントのスピンアップが完了すると、CircleCI Server で Nomad コントロールプレーンを設定するために必要な証明書とキーを出力します。 それらを安全な場所にコピーします。

endif::env-aws[]

[#nomad-autoscaler-configuration]
=== b.  Nomad Autoscaler configuration

Nomad can automatically scale up or down your Nomad clients, provided your clients are managed by a cloud provider's autoscaling resource. With Nomad Autoscaler, you need to provide permission for the utility to manage your autoscaling resource and specify where it is located. CircleCI's Nomad terraform module can provision the permissions resources, or it can be done manually.

ifndef::env-gcp[]

[#aws-iam-role]
==== AWS autoscaler IAM/role

IAM ユーザーまたは Nomad Autoscaler のロールとポリシーを作成します。 You may take **one** of the following approaches:

* The CircleCI link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[nomad module] creates an IAM user and outputs the keys if you set variable `nomad_auto_scaler = true`. 詳細については、リンクの例を参照してください。 既にクライアントを作成済みの場合は、変数をアップデートして `terraform apply` を実行します。 The created user's access and secret key will be available in Terraform's output.
* Create a Nomad Autoscaler IAM user manually with the <<iam-policy-for-nomad-autoscaler,IAM policy below>>. Then, generate an access and secret key for this user.
* You may create a https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[Role for Service Accounts] for Nomad Autoscaler and attach the <<iam-policy-for-nomad-autoscaler,IAM policy below>>:

When using access and secret keys, you have two options for configuration:

[.tab.awsautoscaler.CircleCI_creates_Secret]
--
**Option 1:** CircleCI creates the Kubernetes Secret for you.

Add the access key and secret to your `values.yaml` file with any additional required configuration:

[source, yaml]
----
nomad:
...
  auto_scaler:
    aws:
      accessKey: "<access-key>"
      secretKey: "<secret-key>"
----
--

[.tab.awsautoscaler.You_create_Secret]
--
**Option 2:** Create the Kubernetes Secret yourself

Instead of storing the access key and secret in your `values.yaml` file, you may create the Kubernetes Secret yourself.

NOTE: When using this method, an additional field is required for this secret, as outlined below.

First, add your access key, secret, and region to the following text, and encode it all with base64.

[source,shell]
----
ADDITIONAL_CONFIG=`cat << EOF | base64
target "aws-asg" {
  driver = "aws-asg"
  config = {
    aws_region = "<aws-region>"
    aws_access_key_id = "<access-key>"
    aws_secret_access_key = "<secret-key>"
  }
}
EOF`
----

Then, using that additional base64 encoded config, create the Kubernetes Secret.

[source, shell]
----
# With the base64-encoded additional config from above
kubectl create secret generic nomad-autoscaler-secret \
  --from-literal=secret.hcl=$ADDITIONAL_CONFIG
----
--

[#iam-policy-for-nomad-autoscaler]
===== IAM policy for Nomad Autoscaler

[source, json]
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:CreateOrUpdateTags",
                "autoscaling:UpdateAutoScalingGroup",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "<<Your Autoscaling Group ARN>>"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeScalingActivities",
                "autoscaling:DescribeAutoScalingGroups"
            ],
            "Resource": "*"
        }
    ]
}

endif::env-gcp[]

ifndef::env-aws[]

[#gcp-service-account]
==== GCP autoscaler service account

Create a service account for Nomad Autoscaler. You may take **one** of the following approaches:

[.tab.gcpautoscaler.CircleCI_creates_Secret]
--
**Option 1:** CircleCI creates the Kubernetes Secret.

The CircleCI link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[Nomad module] can create a service account and output a file with the JSON key. For this option, set the variable `nomad_auto_scaler = true`. 詳細については、リンクの例を参照してください。 The created service account key will be available in a file named `nomad-as-key.json`.
--

[.tab.gcpautoscaler.Use_Workload_Identity]
--
**オプション 2: Workload Identity を使用する場合

The CircleCI link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[nomad module] can create a service account using link:/docs/server/installation/phase-1-prerequisites#enable-workload-identity-in-gke[Workload Identity] and out the email. Set the variables `nomad_auto_scaler = true` and `enable_workload_identity = true`.
--

[.tab.gcpautoscaler.You_create_Secret]
--
**Option 3:** Create the Kubernetes Secret yourself.

NOTE: When creating the Kubernetes Secret manually, an additional field is required, as outlined below.

[source, shell]
----
# Base64 encoded additional configuration field
ADDITIONAL_CONFIG=dGFyZ2V0ICJnY2UtbWlnIiB7CiAgZHJpdmVyID0gImdjZS1taWciCiAgY29uZmlnID0gewogICAgY3JlZGVudGlhbHMgPSAiL2V0Yy9ub21hZC1hdXRvc2NhbGVyL2NyZWRzL2djcF9zYS5qc29uIgogIH0KfQo=
kubectl create secret generic nomad-autoscaler-secret \
  --from-literal=gcp_sa.json=<service-account> \
  --from-literal=secret.hcl=$ADDITIONAL_CONFIG
----

When creating a Nomad GCP service account manually, the service account will need the role `compute.admin`. It will also need the role `iam.workloadIdentityUser` if using Workload Identity. This step is only required if you choose not to create the service account using Terraform.
--

endif::env-aws[]

[#nomad-servers]
== 2. Nomad サーバー

Now that you have successfully deployed your Nomad clients and have the permission resources, you can configure the Nomad Servers.

[#nomad-gossip-encryption-key]
=== a.  Nomad gossip encryption key

Nomad requires a key to encrypt communications. This key must be exactly 32 bytes long. 値を紛失した場合、CircleCI が復元することはできません。 Depending on how you prefer to manage Kubernetes Secrets, there are two options:

[.tab.encryption.You_create_Secret]
--
**Option 1:** Create the Kubernetes Secret yourself.

[source,shell]
----
kubectl create secret generic nomad-gossip-encryption-key \
--from-literal=gossip-key=<secret-key-32-chars>
----

Once the Kubernetes Secret exists, no change to `values.yaml` is required. The Kubernetes Secret will be referenced by default.
--

[.tab.encryption.CircleCI_creates_Secret]
--
**Option 2:** CircleCI creates the Kubernetes Secret.

値を `values.yaml` に追加します。 CircleCI will create the Kubernetes Secret automatically.

[source,yaml]
----
nomad:
  server:
    gossip:
      encryption:
        key: <secret-key-32-chars>
----
--

[#nomad-mtls]
=== b.  Nomad mTLS

The `CACertificate`, `certificate` and `privateKey` can be found in the output of the terraform module.  They must be base64 encoded.

[source,yaml]
----
nomad:
  server:
    ...
    rpc:
      mTLS:
        enabled: true
        certificate: <base64-encoded-certificate>
        privateKey: <base64-encoded-private-key>
        CACertificate: <base64-encoded-ca-certificate>
----

[#nomad-autoscaler]
=== c.  Nomad Autoscaler

If you have enabled Nomad Autoscaler, also include the following section under `nomad`:

// Don't include this section in the GCP PDF.

ifndef::env-gcp[]

[#aws]
==== AWS

You created these values in the <<aws-iam-role,Nomad Autoscaler Configuration section>>.

[source,yaml]
----
nomad:
  ...
  auto_scaler:
    enabled: true
    scaling:
      max: <max-node-limit>
      min: <min-node-limit>

    aws:
      enabled: true
      region: <region>
      autoScalingGroup: <asg-name>

      accessKey: <access-key>
      secretKey: <secret-key>
      # or
      irsaRole: <role-arn>
----

// Stop hiding from GCP PDF:

endif::env-gcp[]

// Don't include this section in the AWS PDF:

ifndef::env-aws[]

[#gcp]
==== GCP

You created these values in the <<gcp-service-account,Nomad Autoscaler Configuration section>>.

[source,yaml]
----
nomad:
  ...
  auto_scaler:
    enabled: true
    scaling:
      max: <max-node-limit>
      min: <min-node-limit>

    gcp:
      enabled: true
      project_id: <project-id>
      mig_name: <instance-group-name>

      region: <region>
      # or
      zone: <zone>

      workloadIdentity: <service-account-email>
      # or
      service_account: <service-account-json>
----

// Stop hiding from AWS PDF

endif::env-aws[]

=== d. Helm upgrade

Apply the changes made to your `values.yaml` file:

[source,shell]
----
namespace=<your-namespace>
helm upgrade circleci-server oci://cciserver.azurecr.io/circleci-server -n $namespace --version 4.0.0 -f <path-to-values.yaml>
----

[#nomad-clients-validation]
=== 3. Normad クライアントの確認

CircleCI has created a project called https://github.com/circleci/realitycheck[realitycheck] which allows you to test your server installation. CircleCI はこのプロジェクトをフォローし、システムが期待どおりに動作しているかを確認します。 As you continue through the next phase, sections of realitycheck will move from red (fail) to green (pass).

Before running realitycheck, check if the Nomad servers can communicate with the Nomad clients by executing the below command.

[source,shell]
----
kubectl -n <namespace> exec -it $(kubectl -n <namespace> get pods -l app=nomad-server -o name | tail -1) -- nomad node status
----

You should be able to see output like this:

[source,shell]
----
ID        DC       Name              Class        Drain  Eligibility  Status
132ed55b  default  ip-192-168-44-29  linux-64bit  false  eligible     ready
----

realitycheck を実行するには、リポジトリのクローンを実行する必要があります。 Github の設定に応じて、以下のいずれかを実行します。

[#github-cloud]
==== Github Cloud

[source,shell]
----
git clone https://github.com/circleci/realitycheck.git
----

[#github-enterprise-nomad]
==== GitHub Enterprise

[source,shell]
----
git clone https://github.com/circleci/realitycheck.git
git remote set-url origin <YOUR_GH_REPO_URL>
git push
----

レポジトリのクローンに成功したら、CircleCI Server 内からフォローすることができます。 以下の変数を設定する必要があります。 For full instructions please see the https://github.com/circleci/realitycheck#prerequisites-1[repository readme].

.環境変数
[.table.table-striped]
[cols=2*, options="header", stripes=even]
|===
|名前
|値

|CIRCLE_HOSTNAME
|<YOUR_CIRCLECI_INSTALLATION_URL>

|CIRCLE_TOKEN

|<YOUR_CIRCLECI_API_TOKEN>
|===

.コンテキスト
[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|名前
|環境変数キー
|環境変数値

|org-global
|CONTEXT_END_TO_END_TEST_VAR
|空欄のまま

|individual-local
|MULTI_CONTEXT_END_TO_END_VAR
|空欄のまま
|===

環境変数とコンテキストを設定したら、 realitycheck テストを再実行します。 機能とリソースジョブが正常に完了したことが表示されます。 テスト結果は次のようになります。


image::realitycheck-pipeline.png[Screenshot showing the realitycheck project building in the CircleCI app]

[#vm-service]
== 3. VM サービス

VM service configures virtual machine and remote docker jobs. スケーリング ルールなど、さまざまなオプションを構成することができます。 VM service is unique to AWS and GCP installations because it relies on specific features of these cloud providers.

ifndef::env-gcp[]

[#aws-vm-service]
=== AWS

[#set-up-security-group]
==== Set up security group

. *Get the information needed to create security groups*
+
The following command returns your VPC ID (`vpcId`) and CIDR Block (`serviceIpv4Cidr`) which you need throughout this section:
+
[source,shell]
----
aws eks describe-cluster --name=<your-cluster-name>
----
. *セキュリティーグループを作成します。*
+
以下のコマンドを実行して、VM サービス用のセキュリティーグループを作成します。
+
[source,shell]
----
aws ec2 create-security-group --vpc-id "<VPC_ID>" --description "CircleCI VM Service security group" --group-name "circleci-vm-service-sg"
----
+
これにより次の手順で使用するグループ ID が出力されます。
+
[source, json]
{
    "GroupId": "<VM_SECURITY_GROUP_ID>"
}
.  *セキュリティーグループ Nomad を適用します。*
+
Use the security group you just created, and your CIDR block values, to apply the security group. This allows VM service to communicate with created EC2 instances on port 22.
+
[source,shell]
----
aws ec2 authorize-security-group-ingress --group-id "<VM_SECURITY_GROUP_ID>" --protocol tcp --port 22 --cidr "<SERVICE_IPV4_CIDR>"
----
+
For each https://github.com/CircleCI-Public/server-terraform/blob/main/nomad-aws/variables.tf#L1-L11[subnet] used by the Nomad clients, find the subnet cidr block and add two rules with the following commands.
+
[source,shell]
----
# find CIDR block
aws ec2 describe-subnets --subnet-ids=<NOMAD_SUBNET_ID>
----
+
[source,shell]
----
# add a security group allowing docker access from nomad clients, to VM instances
aws ec2 authorize-security-group-ingress --group-id "<VM_SECURITY_GROUP_ID>" --protocol tcp --port 2376 --cidr "<SUBNET_IPV4_CIDR>"
----
+
[source,shell]
----
# add a security group allowing SSH access from nomad clients, to VM instances
aws ec2 authorize-security-group-ingress --group-id "<VM_SECURITY_GROUP_ID>" --protocol tcp --port 22 --cidr "<SUBNET_IPV4_CIDR>"
----
. *Apply the security group for SSH (If using public IPs for machines)*
+
If using public IPs for VM service instances, run the following command to apply the security group rules so users can SSH into their jobs:
+
[source,shell]
----
aws ec2 authorize-security-group-ingress --group-id "<VM_SECURITY_GROUP_ID>" --protocol tcp --port 54782 --cidr "0.0.0.0/0"
----

[#set-up-authentication]
==== 認証を設定します。

There are two ways to authenticate CircleCI with your cloud provider: IAM Roles for Service Accounts (IRSA), and IAM access keys. IRSA を使用する方法を推奨します。 

[.tab.vmauthaws.IRSA]
--
The following is a summary of https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[AWS's Documentation on IRSA] that is sufficient for configuring VM Service in CircleCI.

. *Create indentity provider*
+
Create an IAM OIDC identity provider for your EKS Cluster:
+
[source,shell]
----
eksctl utils associate-iam-oidc-provider --cluster <CLUSTER_NAME> --approve
----
. *Get ARN*
+
Get the OIDC provider ARN with the following command, you will need it in later steps:
+
[source,shell]
----
aws iam list-open-id-connect-providers | grep $(aws eks describe-cluster --name <CLUSTER_NAME> --query "cluster.identity.oidc.issuer" --output text | awk -F'/' '{print $NF}')
----
. *Get URL*
+
Get your OIDC provider URL, you will need it in later steps
+
[source,shell]
----
aws eks describe-cluster --name <CLUSTER_NAME> --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///"
----
. *Create role*
+
Create the role using the command and trust policy template below, you will need the Role ARN and name in later steps:
+
[source,shell]
----
aws iam create-role --role-name circleci-vm --assume-role-policy-document file://<TRUST_POLICY_FILE>
----
+
[source, json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "<OIDC_PROVIDER_ARN>"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "<OIDC_PROVIDER_URL>:sub": "system:serviceaccount:<K8S_NAMESPACE>:vm-service"
        }
      }
    }

  ]
}
----
. *ポリシーを作成します。*
+
以下のコマンドとテンプレートを使ってポリシーを作成します。  Fill in the security group ID and the VPC ID:
+
[source,shell]
----
aws iam create-policy --policy-name circleci-vm --policy-document file://<POLICY_FILE>
----
+
[source, json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<SECURITY_GROUP_ID>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<VPC_ID>"
        }
      }
    }
  ]
}

----
. *Attach policy*
+
ポリシーをロールにアタッチします。
+
[source,shell]
----
aws iam attach-role-policy --role-name <VM_ROLE_NAME> --policy-arn=<VM_POLICY_ARN>
----
. *Configure VM-Service*
+
Configure VM service by adding the following to `values.yaml`:
+
[source,yaml]
----
vm_service:
  providers:
    ec2:
      enabled: true
      region: <REGION>
      assignPublicIP: true
      irsaRole: <IRSA_ROLE_ARN>
      subnets:
      - <SUBNET_ID>
      securityGroupId: <SECURITY_GROUP_ID>
----
--

[.tab.vmauthaws.IAM_Access_Keys]
--
. *ユーザーを作成します。*
+
プログラムでのアクセス権を持つ新規ユーザーを作成します。
+
[source,shell]
----
aws iam create-user --user-name circleci-vm-service
----
+
vm-service では、オプションで AWS キーの代わりに https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[サービスアカウントのロール]の使用もサポートしています。 ロールを使用する場合は、以下のステップ 6 のポリシーを使って以下の https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[手順]を実行します。
Once done, you may skip to step 9, enabling vm-service.
. *ポリシーを作成します。*
+
以下の内容の `policy.json` ファイルを作成します。 ステップ 2 で作成した VM サービスセキュリティグループの ID (`VMServiceSecurityGroupId`) と VPC ID (`vpcID`) を入力します。
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<YOUR_VMServiceSecurityGroupID>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<YOUR_vpcID>"
        }
      }
    }
  ]
}
----
. *ポリシーをユーザーにアタッチします。*
+
Once you have created the policy.json file, attach it to an IAM policy and created user:
+
[source,shell]
----
aws iam put-user-policy --user-name circleci-vm-service --policy-name circleci-vm-service --policy-document file://policy.json
----
. *ユーザー用のアクセスキーとシークレットを作成します。*
+
作成していない場合は、`circleci-vm-service` ユーザー用のアクセスキーとシークレットが必要です。 以下のコマンドを実行して作成することができます。
+
[source,shell]
----
aws iam create-access-key --user-name circleci-vm-service
----
. *Configure server (there are two options)*
* *Option 1 - Add the keys to `values.yaml`*
Add the VM Service configuration to `values.yaml`. Details of the available configuration options can be found in the link:/docs/server/operator/manage-virtual-machines-with-vm-service[Managing Virtual Machines with VM Service] guide.
* *Option 2 - Create the Kubernetes Secret yourself*
Instead of providing the access key and secret in your `values.yaml` file, you may create the Kubernetes Secret yourself.
+
[source,shell]
----
kubectl create secret generic vm-service-secret \
  --from-literal=ec2AccessKey=<access-key> \
  --from-literal=ec2SecretKey=<secret-key>
----
--

endif::env-gcp[]

ifndef::env-aws[]

[#gcp-authentication]
=== GCP

以下のセクションを完了するにはクラスタに関する追加情報が必要です。 次のコマンドを実行します。

[source,shell]
----
gcloud container clusters describe
----

このコマンドは、次のような情報を返します。この情報には、ネットワーク、リージョン、および次のセクションを完了するために必要なその他の詳細情報が含まれます。

[source, json]
----
addonsConfig:
  gcePersistentDiskCsiDriverConfig:
    enabled: true
  kubernetesDashboard:
    disabled: true
  networkPolicyConfig:
    disabled: true
clusterIpv4Cidr: 10.100.0.0/14
createTime: '2021-08-20T21:46:18+00:00'
currentMasterVersion: 1.20.8-gke.900
currentNodeCount: 3
currentNodeVersion: 1.20.8-gke.900
databaseEncryption:
…
----

. *ファイアウォール ルールを作成します。*
+
以下のコマンドを実行して、GKE の VM サービス用のファイヤーウォール ルールを作成します。
+
[source,shell]
----
gcloud compute firewall-rules create "circleci-vm-service-internal-nomad-fw" --network "<network>" --action allow --source-ranges "0.0.0.0/0" --rules "TCP:22,TCP:2376"
----
+
NOTE: 自動モードを使用した場合は、 https://cloud.google.com/vpc/docs/vpc#ip-ranges[こちらの表]を参照して、リージョンに基づいて Nomad クライアントの CIDR を見つけることができます。
+
[source,shell]
----
gcloud compute firewall-rules create "circleci-vm-service-internal-k8s-fw" --network "<network>" --action allow --source-ranges "<clusterIpv4Cidr>" --rules "TCP:22,TCP:2376"
----
+
[source,shell]
----
gcloud compute firewall-rules create "circleci-vm-service-external-fw" --network "<network>" --action allow --rules "TCP:54782"
----
. *ユーザーを作成します。*
+
VM サービス専用の一意のサービス アカウントを作成することをお勧めします。 コンピューティング インスタンス管理者 (ベータ版) ロールは、VM サービスを運用するための広範な権限を持っています。 If you wish to make permissions more granular, you can use the Compute Instance Admin (beta) role link:https://cloud.google.com/compute/docs/access/iam#compute.instanceAdmin[documentation] as reference.
+
[source,shell]
----
gcloud iam service-accounts create circleci-server-vm --display-name "circleci-server-vm service account"
----
+
NOTE: CircleCI Server を共有 VCP にデプロイする場合は、 VM ジョブを実行するプロジェクトにこのユーザーを作成します。
. *サービスアカウントのメールアドレスを取得します。*
+
[source,shell]
----
gcloud iam service-accounts list --filter="displayName:circleci-server-vm service account" --format 'value(email)'
----
. *ロールをサービスアカウントに適用します。*
+
Apply the Compute Instance Admin (beta) role to the service account:
+
[source,shell]
----
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/compute.instanceAdmin --condition=None
----
+
さらに
+
[source,shell]
----
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/iam.serviceAccountUser --condition=None
----
. *サービスアカウントで Workload Identity を有効にします。*
+
この手順は、GKE で link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity] を使用している場合のみ実行する必要があります。 Steps to enable Workload Identities are provided in link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Phase 1 - Prerequisites].
+
[source,shell]
----
gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/vm-service]"
----
. *Optionally, get JSON Key File*
+
GKE で link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity] を使用している場合、この手順は不要です。
+
以下のコマンドを実行すると、`circleci-server-vm-keyfile` という名前のファイルがローカル作業ディレクトリに作成されます。 このファイルはサーバーインストールを設定する際に必要になります。
+
[source,shell]
----
gcloud iam service-accounts keys create circleci-server-vm-keyfile --iam-account <YOUR_SERVICE_ACCOUNT_EMAIL>
----
. *サーバーを設定します。*
+
When using service account keys for configuring access for the VM service, there are two options.
+
[.tab.configureserver.CircleCI_creates_Secret]
--
**Option 1:** CircleCI creates the Kubernetes Secret.

Add the VM Service configuration to values.yaml. Details of the available configuration options can be found in the link:/docs/server/operator/manage-virtual-machines-with-vm-service[Managing Virtual Machines with VM Service] guide.
--

[.tab.configureserver.You_create_Secret]
--
**Option 1:** Create the Kubernetes Secret yourself.

Instead of providing the service account in your `values.yaml` file, you may create the Kubernetes Secret yourself.

[source,shell]
----
kubectl create secret generic vm-service-secret \
  --from-literal=gcp_sa.json=<access-key>
----
--

endif::env-aws[]

[#vm-service-validation]
=== VM サービスの検証

Apply they changes made to your values.yaml file.

[source,shell]
----
namespace=<your-namespace>
helm upgrade circleci-server oci://cciserver.azurecr.io/circleci-server -n $namespace --version 4.0.0 -f <path-to-values.yaml>
----

CircleCI Server の設定とデプロイが完了したら、VM サービスが適切に動作しているか確認する必要がありあます。 CircleCI Server 内で、realitycheck プロジェクトを再実行できます。 VM サービスジョブは完了しているはずです。 この時点で、すべてのテストが合格しているはずです。

[#runner]
== 4.  ランナー

[#overview]
=== 概要

CircleCI のランナーには、追加のサーバー設定は不要です。 CircleCI Server はランナーと連携する準備ができています。 ただし、ランナーを作成し、CircleCI Server のインストールを認識するようにランナーエージェントを設定する必要があります。 For complete instructions for setting up runner, see the link:/docs/runner-overview[runner documentation].

NOTE: ランナーには組織ごとに１つ名前空間が必要です。 CircleCI Server には複数の組織が存在する場合があります。 CircleCI Server 内に複数の組織が存在する場合、各組織につき１つランナーの名前空間を設定する必要があります。

ifndef::pdf[]

[#next-steps]
== 次のステップ

* link:/docs/server/installation/phase-4-post-installation[Phase 4: Post Installation]
+
endif::pdf[]