---

version:
- Server v3.x
- サーバー管理者
---
= CircleCI Server v3.x インストール ステップ 3

:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

// This doc uses ifdef and ifndef directives to display or hide content specific to Google Cloud Storage (env-gcp) and AWS (env-aws). Currently, this affects only the generated PDFs. To ensure compatability with the Jekyll version, the directives test for logical opposites. For example, if the attribute is NOT env-aws, display this content. For more information, see https://docs.asciidoctor.org/asciidoc/latest/directives/ifdef-ifndef/.

CircleCI サーバー v3.x ビルドサービスのインストールステップを開始する前に、xref:server-3-install-prerequisites.adoc[ステップ 1: 前提条件] と xref:server-3-install.adoc[ステップ 2: コアサービスのインストール]が実行済みであることを確認してください。

.インストール手順のフローチャート ステップ 3
image::server-install-flow-chart-phase3.png[Flow chart showing the installation flow for server 3.x with phase 3 highlighted]

NOTE: 以下のセクションでは、`< >` の中に表示される認証情報の項目にご自身の情報を入力してください。


toc::[]

== ステップ 3: 実行環境のインストール

=== 出力プロセッサ

出力プロセッサは、 Normad クライアントからの出力処理を行います。 システムの速度が低下している場合にスケーリングするための重要なサービスです。 要求に応じてサービスをスケールアップできるよう、出力プロセッサのレプリカセットを増やすことをお勧めします。

名前空間を変更して `kubectl kots admin-console -n <YOUR_CIRCLECI_NAMESPACE>` を実行し、KOTS 管理者コンソールにアクセスします。



設定で次の項目を探して入力します。

. *Output Processor Load Balancer Hostname* -
The following command provides the IP address of the service:
+
kubectl get service output-processor --namespace=<YOUR_CIRCLECI_NAMESPACE>
. *[Save your configuration (設定を保存)]*:  Nomad クライアントの設定の完了後、設定のデプロイと確認を行います。

=== Nomad クライアント

https://circleci.com/docs/2.0/server-3-overview[概要]で述べたように、Nomad は CircleCI が CircleCI ジョブのスケジュール設定 (Nomad サーバー経由) と実行 (Nomad クライアント経由) に使用するワークロードオーケストレーションツールです。 

Nomad クライアントは Kubernetes クラスタの外部にインストールされ、コントロールプレーン（Nomad サーバー）はクラスタ内にインストールされます。 Nomad クライアントと Nomad コントロールプレーン間の通信は、 mTLS によって保護されます。 The mTLS certificate, private key, and certificate authority will be output after you complete installation of the Nomad Clients.

Once completed, you can update your CircleCI server configuration so your Nomad control plane can communicate with your Nomad Clients.

==== Terraform によるクラスタの作成

CircleCI curates Terraform modules to help install Nomad clients in your chosen cloud provider. モジュールは、 AWS と GCP の両方の Terraform 設定ファイル（`main.tf`）例を含む、 https://github.com/CircleCI-Public/server-terraform[パブリックリポジトリ] で参照できます。 `main.tf` を完了するには、クラスタとサーバーのインストールに関する情報が必要です。 以下でその情報を入手する方法を説明します。

NOTE: If you would also like to set up Nomad Autoscaler at this stage, see the <<#nomad-autoscaler-optional,Nomad Autoscaler>> section of this guide, as some of the requirements can be included in this Terraform setup.

// Don't include this section in the GCP PDF:

ifndef::env-gcp[]
===== AWS
You need some information about your cluster and server installation to complete the required fields for the Terraform configuration file (`main.tf`). 変数の完全な例と完全なリストについては、 https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[こちら] を参照してください。

* *Server_endpoint* - You need to know the Nomad Server endpoint, which is the external IP address of the nomad-server-external Loadbalancer. 以下のコマンドで情報を入手します。
+
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
* クラスタの *Subnet ID (subnet)* 、 *VPC ID (vpcId)* 、 *DNS server (dns_server)*:
以下のコマンドを実行して、クラスタの VPC ID ((vpcId)、CIDR (serviceIpv4Cidr)、およびサブネット (サブネットの ID)を入手します。
+
aws eks describe-cluster --name=<YOUR_CLUSTER_NAME>
+
This returns something similar to the following:
+
[source, json]
{...
"resourcesVpcConfig": {
    "subnetIds": [
        "subnet-033a9fb4be69",
        "subnet-04e89f9eef89",
        "subnet-02907d9f35dd",
        "subnet-0fbc63006c5f",
        "subnet-0d683b6f6ba8",
        "subnet-079d0ca04301"
    ],
    "clusterSecurityGroupId": "sg-022c1b544e574",
    "vpcId": "vpc-02fdfff4c",
    "endpointPublicAccess": true,
    "endpointPrivateAccess": false
...
"kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16"
        },
...
}
+
次に、見つけた VPCID を使用して次のコマンドを実行し、クラスタの CIDR ブロックを取得します。 For AWS, the DNS Server is the third IP in your CIDR block (`CidrBlock`), for example your CIDR block might be `10.100.0.0/16`, so the third IP would be `10.100.0.2`.
+
aws ec2 describe-vpcs --filters Name=vpc-id,Values=<YOUR_VPCID>
+
This returns something like the following:
+
[source, json]
{...
"CidrBlock": "192.168.0.0/16",
"DhcpOptionsId": "dopt-9cff",
"State": "available",
"VpcId": "vpc-02fdfff4c"
...}


Once you have filled in the appropriate information, you can deploy your Nomad clients by running the following command from within the directory of the `main.tf` file:

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it outputs the certificates and keys needed for configuring the Nomad control plane in CircleCI server. Copy them somewhere safe. この適用プロセスには 通常 1 分しかかかりません。

// Stop hiding from GCP PDF:
endif::[]

// Don't include this section in the AWS PDF:

ifndef::env-aws[]
===== GCP
You need the IP address of the Nomad control plane (Nomad Server), which was created when you deployed CircleCI Server. 以下のコマンドを実行することで IP アドレスを取得できます。

[source,shell]
----
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
----

You also need the following information:

* Nomad クライアントを実行する GCP プロジェクト
* Nomad クライアントを実行する GCP ゾーン
* Nomad クライアントを実行する GCP リージョン
* Nomad クライアントを実行する GCP ネットワーク
* Nomad クライアントを実行する GCP サブネットワーク

以下の例をローカル環境にコピーして、特定の設定に必要な情報を入力します。

variable "project" {
  type    = string
  default = "<your-project>"
}

variable "region" {
  type    = string
  default = "<your-region>"
}

variable "zone" {
  type    = string
  default = "<your-zone>"
}

variable "network" {
  type    = string
  default = "<your-network-name>"
  # if you are using a shared vpc, provide the network endpoint rather than the name. eg:
  # default = "https://www.googleapis.com/compute/v1/projects/<host-project>/global/networks/<your-network-name>"
}

variable "subnetwork" {
  type    = string
  default = "<your-subnetwork-name>"
  # if you are using a shared vpc, provide the network endpoint rather than the name. eg:
  # default = "https://www.googleapis.com/compute/v1/projects/<service-project>/regions/<your-region>/subnetworks/<your-subnetwork-name>"
}


variable "server_endpoint" {
  type    = string
  default = "<nomad-server-loadbalancer>:4647"
}

variable "nomad_auto_scaler" {
  type        = bool
  default     = false
  description = "If true, terraform will create a service account to be used by nomad autoscaler."
}

variable "enable_workload_identity" {
  type        = bool
  default     = false
  description = "If true, Workload Identity will be used rather than static credentials'"
}

variable "k8s_namespace" {
  type        = string
  default     = "circleci-server"
  description = "If enable_workload_identity is true, provide application k8s namespace"
}

provider "google-beta" {
  project = var.project
  region  = var.region
  zone    = var.zone
}


module "nomad" {
  source = "git::https://github.com/CircleCI-Public/server-terraform.git//nomad-gcp?ref=3.4.0"

  zone            = var.zone
  region          = var.region
  network         = var.network
  subnetwork      = var.subnetwork
  server_endpoint = var.server_endpoint
  machine_type    = "n2-standard-8"
  nomad_auto_scaler         = var.nomad_auto_scaler
  enable_workload_identity  = var.enable_workload_identity
  k8s_namespace             = var.k8s_namespace

  unsafe_disable_mtls    = false
  assign_public_ip       = true
  preemptible            = true
  target_cpu_utilization = 0.50
}

output "module" {
  value = module.nomad
}

適切な情報を入力したら、以下を実行することにより Normad クライアントをデプロイできます。

[source,shell]
----
terraform init
----

[source,shell]
----
terraform plan
----

[source,shell]
----
terraform apply
----

After Terraform is done spinning up the Nomad client(s), it outputs the certificates and key needed for configuring the Nomad control plane in CircleCI server. Copy them somewhere safe.
endif::[]

==== Nomad Autoscaler

Nomad は、クライアントがクラウドプロバイダの自動スケーリングリソースによって管理されている場合、 Nomad クライアントを自動的にスケールアップまたはスケールダウンするユーティリティを提供します。
 With Nomad Autoscaler, you only need to provide permission for the utility to manage your autoscaling resource and specify where it is located. You can enable this resource via KOTS, which deploys the Nomad Autoscaler service along with your Nomad servers. Below, we go through how to set up Nomad Autoscaler for your provider.

NOTE: The maximum and minimum Nomad client count overwrite the corresponding values set when you created your autoscaling group or managed instance group. It is recommended that you keep these values and those used in your Terraform the same so that they do not compete.

If you do not require this service, click the *Save config* button to update your installation and redeploy server.

ifndef::env-gcp[]
===== AWS
.
 IAM ユーザーまたは Nomad Autoscaler のロールとポリシーを作成します。 以下のいづれか方法で実行します。:
  `nomad_auto_scaler = true` を設定すると、 *link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-aws[Nomad モジュール] が IAM ユーザーを作成し、キーを出力します。
 詳細については、リンクの例を参照してください。 If you have already created the clients, you can update the variable and run `terraform apply`. 作成されたユーザーアクセスキーは Terraform の出力で使用できます。
  * 以下の IAM ポリシーを使用して、手動で Nomad Autoscaler IAM ユーザーを作成することも可能です。 Then you need to generate an access and secret key for this user.
  Nomad Autoscaler 用の https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[サービスアカウントのロール] を作成し、次の IAM ポリシーを添付します。

+

[source, json]
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "autoscaling:CreateOrUpdateTags",
                "autoscaling:UpdateAutoScalingGroup",
                "autoscaling:TerminateInstanceInAutoScalingGroup"
            ],
            "Resource": "<<Your Autoscaling Group ARN>>"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeScalingActivities",
                "autoscaling:DescribeAutoScalingGroups"
            ],
            "Resource": "*"
        }
    ]
}
.
 In your KOTS Admin Console, set Nomad Autoscaler to `enabled`.
. Set Max Node Count* - This overwrites what is currently set as the max for your ASG. It is recommended to keep this value and what was set in your Terraform the same.
. Set Min Node Count* - This overwrites what is currently set as the min for your ASG. この値と Terraform で設定された値を変えないことをお勧めします。
. Select cloud provider: `AWS EC2`.
. Add the region of the autoscaling group.
. 以下のいずれかを選択します。 Add the Nomad Autoscaler user's access key and secret key.
.. Or, the Nomad Autoscaler role's ARN.
. Add the name of the autoscaling Group your Nomad clients were created in.
endif::[]

ifndef::env-aws[]
===== GCP
 Nomad Autoscaler のサービスアカウントを作成します。
変数`nomad_auto_scaler = true` と `enable_workload_identity = false` を設定すると、*  link:https://github.com/CircleCI-Public/server-terraform/tree/main/nomad-gcp[Nomad モジュール]  がサービスアカウントを作成し、ファイルとキーを出力します。
 詳細については、リンクの例を参照してください。 既にクライアントを作成済みの場合は、変数をアップデートして `terraform apply` を実行します。 作成されたユーザーのキーは、`nomad-as-key.json` という名前のファイルにあります。 GKE link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload Identities] を使用している場合は、変数を `nomad_auto_scaler = true` と `enable_workload_identity = true` に設定します。
  * Nomad GCP サービスアカウントを手動で作成することも可能です。 サービスアカウントには  `compute.admin` のロールが必要です。 link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[Workload Identity] を使用している場合は、`iam.workloadIdentityUser` ロールも必要です。 Nomad Autoscaler を `enabled` に設定します。 最大 Node 数* を設定します。 最小 Node 数* を設定します。 クラウドプロバイダーを選択します ( `Google Cloud Platform`)。 プロジェクト ID を追加します。 管理対象のインスタンスグループ名を追加します。 インスタンスグループのタイプは、link:https://cloud.google.com/compute/docs/instance-groups/#types_of_managed_instance_groups[ゾーンまたはリージョン].です。
. 以下のいずれかを選択します。 Nomad Autoscaler の GCP サービスアカウントの JSON を指定します。 link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity]を使用している場合は、Nomad Autoscaler サービスアカウントのメールアドレスを使用します。 GCP クラスタで Workload Identity を有効化する手順は、link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[こちら]を参照してください。
.. `nomad-autoscaler` (kubernetes) サービスアカウントの Workload Identity を有効にします。

gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/nomad-autoscaler]"

NOTE: 静的 JSON 認証情報から Workload Identity に切り替える場合は、GCP および CircleCI KOTS 管理者コンソールからキーを削除する必要があります。
endif::[]

==== 設定とデプロイ

Nomad クライアントの導入が完了したら、 CircleCI Server と Nomad コントロールプレーンを設定できます。 Access the KOTS Admin Console by running the following command, substituting your namespace: `kubectl kots admin-console -n <YOUR_CIRCLECI_NAMESPACE>`

設定で次の項目を入力します。

* *Nomad Load Balancer (Normad ロードバランサー)(必須)*
+
kubectl get service nomad-server-external --namespace=<YOUR_CIRCLECI_NAMESPACE>
* *[Nomad Server Certificate (Nomad サーバーの証明書)](必須)*:
 `terraform apply` からの出力で提供されます。
* *[Nomad Server Private Key (Nomad サーバーのプライベートキー)](必須)*:
 `terraform apply` からの出力で提供されます。
* *Nomad Server Certificate Authority (Nomad サーバーの証明書認証局)](必須)*:
`terraform apply` からの出力で提供されます。
* *Build Agent Image (ビルドエージェントイメージ)* :  カスタム Docker レジストリを使用して CircleCI ビルドエージェントを提供する場合は、カスタマサポートにお問い合わせください。

*[Save config (構成の保存)]* ボタンをクリックし、CircleCI Server を更新して再デプロイします。

==== Normad クライアントの確認

CircleCI Server のインストールをテストできる https://github.com/circleci/realitycheck/tree/server-3.0[realitycheck] というプロジェクトを作成しました。 CircleCI はこのプロジェクトをフォローし、システムが期待どおりに動作しているかを確認します。 引き続き次のステップを実行すると、 realitycheck のセクションが赤から緑に変わります。

To run realitycheck, you need to clone the repository. Depending on your GitHub setup, you can use one of the following commands:

===== GitHub Cloud

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git

----

===== GitHub Enterprise

[source,shell]
----
git clone -b server-3.0 https://github.com/circleci/realitycheck.git
git remote set-url origin <YOUR_GH_REPO_URL>
git push
----

レポジトリのクローンに成功したら、CircleCI Server 内からフォローすることができます。 You need to set the following variables. 詳細は、 https://github.com/circleci/realitycheck/tree/server-3.0[リポジトリ README]を参照してください。

.環境変数
[.table.table-striped]
[cols=2*, options="header", stripes=even]
|===
|名前
|値

|CIRCLE_HOSTNAME
|<YOUR_CIRCLECI_INSTALLATION_URL>

|CIRCLE_TOKEN

|<YOUR_CIRCLECI_API_TOKEN>
|===

.コンテキスト
[.table.table-striped]
[cols=3*, options="header", stripes=even]
|===
|名前
|環境変数キー
|環境変数値

|org-global
|CONTEXT_END_TO_END_TEST_VAR
|空欄のまま

|individual-local
|MULTI_CONTEXT_END_TO_END_VAR
|空欄のまま
|===

環境変数とコンテキストを設定したら、 realitycheck テストを再実行します。 機能とリソースジョブが正常に完了したことが表示されます。 テスト結果は次のようになります。


image::realitycheck-pipeline.png[Screenshot showing the realitycheck project building in the CircleCI app]

=== VM サービス

VM サービスは、VM とリモート Docker ジョブを設定します。 スケーリング ルールなど、さまざまなオプションを構成することができます。 VM サービスは、 EKS および GKE のインストールに固有のものです。これは、これらのクラウドプロバイダーの機能に特に依存しているためです。

ifndef::env-gcp[]
==== AWS
. *セキュリティグループの作成に必要な情報を入手します*。

+
The following command returns your VPC ID (`vpcId`), CIDR Block (`serviceIpv4Cidr`), Cluster Security Group ID (`clusterSecurityGroupId`) and Cluster ARN (`arn`) values, which you need throughout this section:

+

aws eks describe-cluster --name=<your-cluster-name>

. *セキュリティーグループを作成します。*
+
Run the following commands to create a security group for VM service:
+
aws ec2 create-security-group --vpc-id "<YOUR_VPCID>" --description "CircleCI VM Service security group" --group-name "circleci-vm-service-sg"
+
This outputs a GroupID to be used in the next steps:
+
[source, json]
{
    "GroupId": "sg-0cd93e7b30608b4fc"
}
.  *セキュリティーグループ Nomad を適用します。*
+
作成したセキュリティーグループと CIDR ブロック値を使ってセキュリティーグループを以下に適用します。
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 22 --cidr "<YOUR_serviceIpv4Cidr>"
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 2376 --cidr "<YOUR_serviceIpv4Cidr>"
+
NOTE: If you created your Nomad Clients in a different subnet from CircleCI server, you need to rerun the above two commands with each subnet CIDR.
. *セキュリティーグループに SSH接続を適用します。*
+
次のコマンドを実行してセキュリティグループルールを適用し、ユーザーがジョブに SSH 接続できるようにします。
+
aws ec2 authorize-security-group-ingress --group-id "<YOUR_GroupId>" --protocol tcp --port 54782
. *ユーザーを作成します。*
+
プログラムでのアクセス権を持つ新規ユーザーを作成します。
+
aws iam create-user --user-name circleci-vm-service
+
vm-service では、オプションで AWS キーの代わりに https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[サービスアカウントのロール]の使用もサポートしています。 ロールを使用する場合は、以下のステップ 6 のポリシーを使って以下の https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[手順]を実行します。
完了したら、ステップ 9 に進みます。手順 9 では、KOTS で VM サービスを有効化します。
. *ポリシーを作成します。*
+
以下の内容の `policy.json` ファイルを作成します。 ステップ 2 で作成した VM サービスセキュリティグループの ID (`VMServiceSecurityGroupId`) と VPC ID (`vpcID`) を入力します。
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*::image/*",
        "arn:aws:ec2:*::snapshot/*",
        "arn:aws:ec2:*:*:key-pair/*",
        "arn:aws:ec2:*:*:launch-template/*",
        "arn:aws:ec2:*:*:network-interface/*",
        "arn:aws:ec2:*:*:placement-group/*",
        "arn:aws:ec2:*:*:volume/*",
        "arn:aws:ec2:*:*:subnet/*",
        "arn:aws:ec2:*:*:security-group/<YOUR_VMServiceSecurityGroupID>"
      ]
    },
    {
      "Action": "ec2:RunInstances",
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:instance/*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateVolume"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:ec2:*:*:volume/*"
      ],
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:Describe*"
      ],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "CreateVolume"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateTags"
      ],
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:CreateAction" : "RunInstances"
        }
      }
    },
    {
      "Action": [
        "ec2:CreateTags",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances",
        "ec2:AttachVolume",
        "ec2:DetachVolume",
        "ec2:DeleteVolume"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:*/*",
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/ManagedBy": "circleci-vm-service"
        }
      }
    },
    {
      "Action": [
        "ec2:RunInstances",
        "ec2:StartInstances",
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:ec2:*:*:subnet/*",
      "Condition": {
        "StringEquals": {
          "ec2:Vpc": "<YOUR_vpcID>"
        }
      }
    }
  ]
}
----
. *ポリシーをユーザーにアタッチします。*
+
Once you have created the policy.json file, attach it to an IAM policy and created user.
+
aws iam put-user-policy --user-name circleci-vm-service --policy-name circleci-vm-service --policy-document file://policy.json
. *ユーザー用のアクセスキーとシークレットを作成します。*
+
If you have not already created them, you will need an access key and secret for the `circleci-vm-service` user. You can create those by running the following command:
+
aws iam create-access-key --user-name circleci-vm-service
. *サーバーを設定します。*
+
Configure VM Service through the KOTS Admin Console. 利用可能な設定オプションの詳細については、 https://circleci.com/docs/2.0/server-3-operator-vm-service[VM サービス]ガイドを参照してください。
+
フィールドの設定が完了したら、*設定を保存し*、更新したアプリケーションをデプロイします。
endif::[]

ifndef::env-aws[]
==== GCP

You need additional information about your cluster to complete the next section. 次のコマンドを実行します。

gcloud container clusters describe

This command returns something like the following, which includes network, region and other details that you need to complete the next section:

[source, json]
----
addonsConfig:
  gcePersistentDiskCsiDriverConfig:
    enabled: true
  kubernetesDashboard:
    disabled: true
  networkPolicyConfig:
    disabled: true
clusterIpv4Cidr: 10.100.0.0/14
createTime: '2021-08-20T21:46:18+00:00'
currentMasterVersion: 1.20.8-gke.900
currentNodeCount: 3
currentNodeVersion: 1.20.8-gke.900
databaseEncryption:
…
----

. *ファイアウォール ルールを作成します。*
+
以下のコマンドを実行して、GKE の VM サービス用のファイヤーウォール ルールを作成します。
+
gcloud compute firewall-rules create "circleci-vm-service-internal-nomad-fw" --network "<network>" --action allow --source-ranges "0.0.0.0/0" --rules "TCP:22,TCP:2376"
+
NOTE: If you have used auto-mode, you can find the Nomad clients CIDR based on the region by referring to the https://cloud.google.com/vpc/docs/vpc#ip-ranges[table here].
+
gcloud compute firewall-rules create "circleci-vm-service-internal-k8s-fw" --network "<network>" --action allow --source-ranges "<clusterIpv4Cidr>" --rules "TCP:22,TCP:2376"
+
gcloud compute firewall-rules create "circleci-vm-service-external-fw" --network "<network>" --action allow --rules "TCP:54782"
. *ユーザーを作成します。*
+
VM サービス専用の一意のサービス アカウントを作成することをお勧めします。 コンピューティング インスタンス管理者 (ベータ版) ロールは、VM サービスを運用するための広範な権限を持っています。 アクセス許可をより詳細に設定したい場合は、 コンピューティング インスタンス管理者 (ベータ版) ロールのドキュメントを参照してください。
+
gcloud iam service-accounts create circleci-server-vm --display-name "circleci-server-vm service account"
+
NOTE: If your are deploying CircleCI server in a shared VCP, you should create this user in the project in which you intend to run your VM jobs.
. *サービスアカウントのメールアドレスを取得します。*
+
gcloud iam service-accounts list --filter="displayName:circleci-server-vm service account" --format 'value(email)'
. *ロールをサービスアカウントに適用します。*
+
Apply the Compute Instance Admin (Beta) role to the service account:
+
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/compute.instanceAdmin --condition=None
+
さらに
+
gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> --member serviceAccount:<YOUR_SERVICE_ACCOUNT_EMAIL> --role roles/iam.serviceAccountUser --condition=None
. *JSON キーファイルを取得します。*
+
GKE で link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity] を使用している場合、この手順は不要です。
+
以下のコマンドを実行すると、`circleci-server-vm-keyfile` という名前のファイルがローカル作業ディレクトリに作成されます。 サーバーインストールを設定する際に必要になります。
+
gcloud iam service-accounts keys create circleci-server-vm-keyfile --iam-account <YOUR_SERVICE_ACCOUNT_EMAIL>
. *サービスアカウントで Workload Identity を有効にします。*
+
この手順は、GKE で link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity] を使用している場合のみ実行する必要があります。 Workload Identity を有効化する手順は、link:https://circleci.com/docs/2.0/server-3-install-prerequisites/index.html#enabling-workload-identity-in-gke[こちら]を参照してください。
+
gcloud iam service-accounts add-iam-policy-binding <YOUR_SERVICE_ACCOUNT_EMAIL> \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:<GCP_PROJECT_ID>.svc.id.goog[circleci-server/vm-service]"

NOTE: 静的 JSON 認証情報から Workload Identity に切り替える場合は、GCP および CircleCI KOTS 管理者コンソールからキーを削除する必要があります。

. *サーバーを設定します。*
+
Configure VM Service through the KOTS Admin Console. 利用可能な設定オプションの詳細については、 https://circleci.com/docs/2.0/server-3-operator-vm-service[VM サービス]ガイドを参照してください。
+
フィールドの設定が完了したら、*設定を保存し*、更新したアプリケーションをデプロイします。
endif::[]

==== VM サービスの検証

Once you have configured and deployed CircleCI server, you should validate that VM Service is operational. You can rerun the realitychecker project within your CircleCI installation and you should see the VM Service Jobs complete. At this point, all tests should pass.

=== ランナーのバージョン

==== 概要

CircleCI のランナーには、追加のサーバー設定は不要です。 CircleCI Server はランナーと連携する準備ができています。 However, you need to create a runner and configure the runner agent to be aware of your server installation. For complete instructions for setting up runner, see the link:https://circleci.com/docs/2.0/runner-overview/?section=executors-and-images[runner documentation].

NOTE: ランナーには各組織につき１つ名前空間が必要です。  CircleCI Server には複数の組織が存在する場合があります。 If your company has multiple organizations within your CircleCI installation, you need to set up a runner namespace for each organization within your server installation.

ifndef::pdf[]
## 次に読む

* https://circleci.com/docs/2.0/server-3-install-post[Server 3.x ステップ 4 - ポストインストール]
* https://circleci.com/docs/2.0/server-3-install-hardening-your-cluster[クラスタのハードニング]
* https://circleci.com/docs/2.0/server-3-install-migration[Server 3.x への移行]
endif::[]